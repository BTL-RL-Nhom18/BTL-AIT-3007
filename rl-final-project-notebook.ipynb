{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:15:51.153403Z",
     "iopub.status.busy": "2024-12-16T16:15:51.152962Z",
     "iopub.status.idle": "2024-12-16T16:16:01.281885Z",
     "shell.execute_reply": "2024-12-16T16:16:01.280385Z",
     "shell.execute_reply.started": "2024-12-16T16:15:51.153357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: magent2 in /opt/conda/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2) (1.26.4)\n",
      "Requirement already satisfied: pygame>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from magent2) (2.6.1)\n",
      "Requirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2) (1.24.0)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2) (0.29.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Install magent2\n",
    "!pip install git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from magent2.environments import battle_v4\n",
    "from torch_model import QNetwork  # Đảm bảo rằng bạn đã tải lên file torch_model.py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Định nghĩa lớp ReplayBuffer\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        state, action, reward, next_state, done = self.buffer[idx]\n",
    "        return (\n",
    "            torch.FloatTensor(state).permute(2, 0, 1),\n",
    "            torch.LongTensor([action]),\n",
    "            torch.FloatTensor([reward]), \n",
    "            torch.FloatTensor(next_state).permute(2, 0, 1),\n",
    "            torch.FloatTensor([done])\n",
    "        )\n",
    "\n",
    "# Khởi tạo môi trường và thiết bị\n",
    "env = battle_v4.env(map_size=45, max_cycles=300)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Khởi tạo Q-networks\n",
    "q_network = QNetwork(\n",
    "    env.observation_space(\"blue_0\").shape,\n",
    "    env.action_space(\"blue_0\").n\n",
    ").to(device)\n",
    "\n",
    "target_network = QNetwork(\n",
    "    env.observation_space(\"blue_0\").shape,\n",
    "    env.action_space(\"blue_0\").n\n",
    ").to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "# Random policy for red agents\n",
    "def random_policy():\n",
    "    return env.action_space(\"red_0\").sample()\n",
    "\n",
    "# Khởi tạo Optimizer và Replay Buffer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Thiết lập hyperparameters\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "target_update_freq = 1000\n",
    "train_freq = 4\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "epsilon = epsilon_start\n",
    "num_episodes = 800\n",
    "step_count = 0\n",
    "\n",
    "# Hàm chọn hành động cho agent xanh\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space(\"blue_0\").sample()\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "# Hàm tối ưu mô hình\n",
    "def optimize_model():\n",
    "    train_loader = DataLoader(\n",
    "        replay_buffer,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    for states, actions, rewards, next_states, dones in train_loader:\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device) \n",
    "        dones = dones.to(device)\n",
    "\n",
    "        q_values = q_network(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = F.mse_loss(q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Vòng lặp huấn luyện chính\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    done = {agent: False for agent in env.agents}\n",
    "    observations = {}  # Track observations\n",
    "\n",
    "    while not all(done.values()):\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, _ = env.last()\n",
    "            agent_team = agent.split(\"_\")[0]\n",
    "            \n",
    "            # Track observations\n",
    "            if agent not in observations:\n",
    "                observations[agent] = obs\n",
    "            next_obs = observations.get(agent, obs)  # Use tracked observation\n",
    "\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                done[agent] = True\n",
    "            else:\n",
    "                if agent_team == \"blue\":\n",
    "                    action = select_action(obs, epsilon)\n",
    "                    replay_buffer.add(obs, action, reward, next_obs, termination or truncation)\n",
    "                    observations[agent] = next_obs  # Update observation\n",
    "                    step_count += 1\n",
    "                    total_reward += reward\n",
    "                else:\n",
    "                    action = random_policy()\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        # Optimize outside agent loop\n",
    "        if step_count % train_freq == 0:\n",
    "            optimize_model()\n",
    "\n",
    "    # Cập nhật epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # Cập nhật target network\n",
    "    if step_count % target_update_freq == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    # Lưu checkpoint mô hình\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        checkpoint_path = f\"blue_vs_final_checkpoint_{episode + 1}.pt\"\n",
    "        torch.save(q_network.state_dict(), checkpoint_path)\n",
    "        print(f\"Model checkpoint saved at episode {episode + 1}\")\n",
    "\n",
    "# Lưu mô hình đã huấn luyện\n",
    "torch.save(q_network.state_dict(), \"blue_vs_random.pt\")\n",
    "print(\"Training complete. Model saved as 'blue_vs_random.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6316483,
     "sourceId": 10218330,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6316592,
     "sourceId": 10218468,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
