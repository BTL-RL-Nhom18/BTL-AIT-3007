{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af52205e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T16:54:39.237557Z",
     "iopub.status.busy": "2024-12-18T16:54:39.237163Z",
     "iopub.status.idle": "2024-12-18T16:55:18.633894Z",
     "shell.execute_reply": "2024-12-18T16:55:18.632402Z"
    },
    "papermill": {
     "duration": 39.404578,
     "end_time": "2024-12-18T16:55:18.636207",
     "exception": false,
     "start_time": "2024-12-18T16:54:39.231629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c480f9",
   "metadata": {
    "papermill": {
     "duration": 0.003816,
     "end_time": "2024-12-18T16:55:18.644501",
     "exception": false,
     "start_time": "2024-12-18T16:55:18.640685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae961393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:18.653927Z",
     "iopub.status.busy": "2024-12-18T16:55:18.653558Z",
     "iopub.status.idle": "2024-12-18T16:55:22.630348Z",
     "shell.execute_reply": "2024-12-18T16:55:22.629395Z"
    },
    "papermill": {
     "duration": 3.984193,
     "end_time": "2024-12-18T16:55:22.632507",
     "exception": false,
     "start_time": "2024-12-18T16:55:18.648314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from magent2.environments import battle_v4\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import collections\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a2444",
   "metadata": {
    "papermill": {
     "duration": 0.003856,
     "end_time": "2024-12-18T16:55:22.640499",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.636643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c6139e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.651937Z",
     "iopub.status.busy": "2024-12-18T16:55:22.650982Z",
     "iopub.status.idle": "2024-12-18T16:55:22.735621Z",
     "shell.execute_reply": "2024-12-18T16:55:22.734822Z"
    },
    "papermill": {
     "duration": 0.093188,
     "end_time": "2024-12-18T16:55:22.737602",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.644414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "seed = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_output_dim(input_dim, kernel_size, stride, padding):\n",
    "    return (input_dim - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "def save_model(model, name):\n",
    "    torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "def save_data(data, name='data'):\n",
    "    np.save(f'{name}.npy', data)\n",
    "\n",
    "def reseed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reseed(seed)\n",
    "\n",
    "@dataclass\n",
    "class VdnHyperparameters:\n",
    "    lr: float = 0.001\n",
    "    gamma: float = 0.99\n",
    "    batch_size: int = 2048\n",
    "    update_iter: int = 20\n",
    "    buffer_limit: int = 9000\n",
    "    update_target_interval: int = 20\n",
    "    max_episodes: int = 500\n",
    "    max_epsilon: float = 0.9\n",
    "    min_epsilon: float = 0.1\n",
    "    episode_min_epsilon: int = 200\n",
    "    test_episodes: int = 10\n",
    "    warm_up_steps: int = 3000\n",
    "    chunk_size: int = 1\n",
    "    recurrent: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc88cb2",
   "metadata": {
    "papermill": {
     "duration": 0.005182,
     "end_time": "2024-12-18T16:55:22.748762",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.743580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb35d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.759061Z",
     "iopub.status.busy": "2024-12-18T16:55:22.758273Z",
     "iopub.status.idle": "2024-12-18T16:55:22.769531Z",
     "shell.execute_reply": "2024-12-18T16:55:22.768664Z"
    },
    "papermill": {
     "duration": 0.01835,
     "end_time": "2024-12-18T16:55:22.771440",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.753090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ReplayBuffer\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        \"\"\"Update buffer with a new transition\n",
    "        :param transition: tuple of (state, action, reward, next_state, done)\n",
    "        \"\"\"\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_chunk(self, batch_size, chunk_size):\n",
    "        \"\"\"Sample a batch of chunk_size transitions from the buffer\n",
    "        :param batch_size: number of transitions to sample\n",
    "        :param chunk_size: length of horizon of each batch\n",
    "        :return: tuple of (states, actions, rewards, next_states, dones),\n",
    "        their shapes are respectively:\n",
    "        [batch_size, chunk_size, n_agents, ...obs_shape],\n",
    "        [batch_size, chunk_size, n_agents],\n",
    "        [batch_size, chunk_size, n_agents],\n",
    "        [batch_size, chunk_size, n_agents, ...obs_shape],\n",
    "        [batch_size, chunk_size, n_agents]\n",
    "        \"\"\"\n",
    "        start_idx = np.random.randint(0, len(self.buffer) - chunk_size, batch_size)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "\n",
    "        for idx in start_idx:\n",
    "            for chunk_step in range(idx, idx + chunk_size):\n",
    "                # (state, action, reward, next_state, done) * num_agent\n",
    "                s, a, r, s_prime, done = self.buffer[chunk_step]\n",
    "                s_lst.append(s)\n",
    "                a_lst.append(a)\n",
    "                r_lst.append(r)\n",
    "                s_prime_lst.append(s_prime)\n",
    "                done_lst.append(done)\n",
    "        num_agents = len(s_lst[0])\n",
    "        obs_shape = s_lst[0][0].shape\n",
    "\n",
    "        s_lst = np.array(s_lst).reshape(batch_size, chunk_size, num_agents, *obs_shape)\n",
    "        a_lst = np.array(a_lst).reshape(batch_size, chunk_size, num_agents)\n",
    "        r_lst = np.array(r_lst).reshape(batch_size, chunk_size, num_agents)\n",
    "        s_prime_lst = np.array(s_prime_lst).reshape(batch_size, chunk_size, num_agents, *obs_shape)\n",
    "        done_lst = np.array(done_lst, dtype=bool).reshape(batch_size, chunk_size, num_agents)\n",
    "        return (\n",
    "            torch.tensor(s_lst, dtype=torch.float32).to(device),\n",
    "            torch.tensor(a_lst, dtype=torch.float32).to(device),\n",
    "            torch.tensor(r_lst, dtype=torch.float32).to(device),\n",
    "            torch.tensor(s_prime_lst, dtype=torch.float32).to(device),\n",
    "            torch.tensor(done_lst, dtype=torch.float32).to(device)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8f9f8",
   "metadata": {
    "papermill": {
     "duration": 0.003804,
     "end_time": "2024-12-18T16:55:22.779334",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.775530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TeamManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321b7e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.788849Z",
     "iopub.status.busy": "2024-12-18T16:55:22.788518Z",
     "iopub.status.idle": "2024-12-18T16:55:22.804308Z",
     "shell.execute_reply": "2024-12-18T16:55:22.803492Z"
    },
    "papermill": {
     "duration": 0.022842,
     "end_time": "2024-12-18T16:55:22.806118",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.783276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TeamManager\n",
    "\n",
    "class TeamManager:\n",
    "\n",
    "    def __init__(self, agents, my_team = None):\n",
    "        self.agents = agents\n",
    "        self.teams = self.group_agents()\n",
    "        self.terminated_agents = set()\n",
    "        self.my_team = my_team\n",
    "        self.random_agents = None\n",
    "        self.get_random_agents(1)\n",
    "\n",
    "    def get_teams(self):\n",
    "        \"\"\"\n",
    "        Get the team names.\n",
    "        :return: a list of team names\n",
    "        \"\"\"\n",
    "        return list(self.teams.keys())\n",
    "\n",
    "    def get_my_team(self):\n",
    "        if self.my_team is not None:\n",
    "            return self.my_team\n",
    "        else:\n",
    "            my_team = self.get_teams()[1]\n",
    "        self.my_team = my_team\n",
    "        return my_team\n",
    "\n",
    "    def get_other_team(self):\n",
    "        return self.get_teams()[0]\n",
    "\n",
    "    def get_team_agents(self, team):\n",
    "        \"\"\"\n",
    "        Get the agents in a team.\n",
    "        :param team: the team name\n",
    "        :return: a list of agent names in the team\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        return self.teams[team]\n",
    "\n",
    "    def get_my_agents(self):\n",
    "        return self.get_team_agents(self.get_my_team())\n",
    "\n",
    "    def get_other_agents(self):\n",
    "        return self.get_team_agents(self.get_other_team())\n",
    "\n",
    "    def group_agents(self):\n",
    "        \"\"\"\n",
    "        Group agents by their team.\n",
    "        :param agents: a list of agent names in the format of teamname_agentid\n",
    "        :return: a dictionary with team names as keys and a list of agent names as values\n",
    "        \"\"\"\n",
    "        teams = collections.defaultdict(list)\n",
    "        for agent in self.agents:\n",
    "            team, _ = agent.split('_')\n",
    "            teams[team].append(agent)\n",
    "        return teams\n",
    "\n",
    "    def get_info_of_team(self, team, data, default=None):\n",
    "        \"\"\"\n",
    "        Get the information of a team.\n",
    "        :param team: the team name\n",
    "        :param data: the data to get information from\n",
    "        :return: a dictionary with the team name as key and the information as value\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        result = {}\n",
    "        for agent in self.get_team_agents(team):\n",
    "            if agent not in data:\n",
    "                result[agent] = default\n",
    "            else:\n",
    "                result[agent] = data[agent]\n",
    "        return result\n",
    "    \n",
    "    def reset(self):\n",
    "        self.terminated_agents = set()\n",
    "\n",
    "    def is_team_terminated(self, team):\n",
    "        \"\"\"\n",
    "        Check if all agents in a team are terminated.\n",
    "        :param team: the team name\n",
    "        :return: True if all agents in the team are terminated, False otherwise\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        return all(agent in self.terminated_agents for agent in self.teams[team])\n",
    "\n",
    "    def terminate_agent(self, agent):\n",
    "        \"\"\"\n",
    "        Mark an agent as terminated.\n",
    "        :param agent:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.terminated_agents.add(agent)\n",
    "\n",
    "    def has_terminated_teams(self):\n",
    "        \"\"\"\n",
    "        Check if any team is terminated.\n",
    "        \"\"\"\n",
    "        for team in self.teams:\n",
    "            if self.is_team_terminated(team):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_my_terminated_agents(self):\n",
    "        return list(self.terminated_agents.intersection(self.get_my_agents()))\n",
    "\n",
    "    def get_other_team_remains(self):\n",
    "        \"\"\"\n",
    "        Get the remaining agents in the other team.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        my_team = self.get_my_team()\n",
    "        other_team = [team for team in self.teams if team != my_team][0]\n",
    "        return [agent for agent in self.get_team_agents(other_team) if agent not in self.terminated_agents]\n",
    "\n",
    "\n",
    "    def get_random_agents(self, rate):\n",
    "        \"\"\"\n",
    "        Create a random agent list, and return the first n agents.\n",
    "        :param rate: the rate of random agents to return\n",
    "        :return: a list of random agents with the length of rate * num_agents\n",
    "        \"\"\"\n",
    "        num_agents = len(self.get_my_agents())\n",
    "        if self.random_agents is not None:\n",
    "            num_random_agents = int(num_agents * rate)\n",
    "            return self.random_agents[:num_random_agents]\n",
    "        else:\n",
    "            self.random_agents = random.sample(self.get_my_agents(), num_agents)\n",
    "            return self.get_random_agents(rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_terminates_truncates(terminates, truncates):\n",
    "        \"\"\"\n",
    "        Merge terminates and truncates into one dictionary.\n",
    "        :param terminates: a dictionary with agent names as keys and boolean values as values\n",
    "        :param truncates: a dictionary with agent names as keys and boolean values as values\n",
    "        :return: a dictionary with agent names as keys and boolean values as values\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for agent in terminates:\n",
    "            result[agent] = terminates[agent] or truncates[agent]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a2fd7",
   "metadata": {
    "papermill": {
     "duration": 0.003757,
     "end_time": "2024-12-18T16:55:22.814002",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.810245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Code train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1370f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.823717Z",
     "iopub.status.busy": "2024-12-18T16:55:22.823240Z",
     "iopub.status.idle": "2024-12-18T16:55:22.836850Z",
     "shell.execute_reply": "2024-12-18T16:55:22.835957Z"
    },
    "papermill": {
     "duration": 0.020772,
     "end_time": "2024-12-18T16:55:22.838723",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.817951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train(q, q_target, memory, optimizer, gamma, batch_size, update_iter=10, chunk_size=10, grad_clip_norm=5):\n",
    "    q.train()\n",
    "    q_target.eval()\n",
    "    chunk_size = chunk_size if q.recurrent else 1\n",
    "    losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for i in range(update_iter):\n",
    "        # Get data from buffer\n",
    "        states, actions, rewards, next_states, dones = memory.sample_chunk(batch_size, chunk_size)\n",
    "\n",
    "        hidden = q.init_hidden(batch_size).to(device)\n",
    "        target_hidden = q_target.init_hidden(batch_size).to(device)\n",
    "        \n",
    "        loss = 0\n",
    "        for step_i in range(chunk_size):\n",
    "            with autocast():\n",
    "                q_out, hidden = q(states[:, step_i].to(device), hidden)  # [batch_size, num_agents, n_actions]\n",
    "                q_out = q_out.to(device)\n",
    "                hidden = hidden.to(device)\n",
    "                q_a = q_out.gather(2, actions[:, step_i, :].unsqueeze(-1).long().to(device)).squeeze(-1)  # [batch_size, num_agents]: q values of actions taken\n",
    "                sum_q = (q_a * (1 - dones[:, step_i].to(device))).sum(dim=1, keepdims=True)  # [batch_size, 1]\n",
    "    \n",
    "                with torch.no_grad():\n",
    "                    max_q_prime, target_hidden = q_target(next_states[:, step_i].to(device), target_hidden.detach())\n",
    "                    target_hidden = target_hidden.to(device)\n",
    "                    max_q_prime = max_q_prime.max(dim=2)[0].squeeze(-1)  # [batch_size, num_agents]\n",
    "                    target_q = rewards[:, step_i, :].to(device).sum(dim=1, keepdims=True)  # [batch_size, 1]\n",
    "                    target_q += gamma * ((1 - dones[:, step_i].to(device)) * max_q_prime.to(device)).sum(dim=1, keepdims=True)\n",
    "            \n",
    "                loss += F.smooth_l1_loss(sum_q, target_q.detach())\n",
    "            \n",
    "                # Create a mask for each agent separately\n",
    "                done_mask = dones[:, step_i].to(device).bool()  # Shape: (batch_size, num_agents)\n",
    "                \n",
    "                # Lấy chỉ số batch và agent nơi done_mask == 1\n",
    "                batch_indices, agent_indices = torch.where(done_mask)\n",
    "                \n",
    "                # Số lượng agents đã kết thúc\n",
    "                num_terminated = len(batch_indices)\n",
    "                \n",
    "                if num_terminated > 0:  # Chỉ xử lý nếu có agent nào bị kết thúc\n",
    "                    # Khởi tạo hidden states mới cho tất cả các agents bị kết thúc\n",
    "                    new_hidden = q.init_hidden(batch_size=num_terminated).to(device)  # Shape: (num_terminated, num_agents, hx_size)\n",
    "                    new_target_hidden = q_target.init_hidden(batch_size=num_terminated).to(device)  # Same shape\n",
    "                \n",
    "                    # Lấy hidden states tương ứng với từng agent\n",
    "                    new_hidden_agents = new_hidden[range(num_terminated), agent_indices, :]  # Shape: (num_terminated, hx_size)\n",
    "                    new_target_hidden_agents = new_target_hidden[range(num_terminated), agent_indices, :]  # Same shape\n",
    "                \n",
    "                    # Gán các hidden states mới vào các vị trí tương ứng trong tensor `hidden` và `target_hidden`\n",
    "                    hidden[batch_indices, agent_indices, :] = new_hidden_agents\n",
    "                    target_hidden[batch_indices, agent_indices, :] = new_target_hidden_agents\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(q.parameters(), grad_clip_norm, norm_type=2)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    print('Loss: ' + \" \".join([str(round(loss, 2)) for loss in losses]))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26141244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.848320Z",
     "iopub.status.busy": "2024-12-18T16:55:22.848016Z",
     "iopub.status.idle": "2024-12-18T16:55:22.861443Z",
     "shell.execute_reply": "2024-12-18T16:55:22.860519Z"
    },
    "papermill": {
     "duration": 0.020415,
     "end_time": "2024-12-18T16:55:22.863247",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.842832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run_episode\n",
    "\n",
    "def run_episode(env, q, opponent_q, memory=None, random_rate=0, epsilon=0.1):\n",
    "    \"\"\"Run an episode in self-play mode\n",
    "    :return: total score of the episode\n",
    "    \"\"\"\n",
    "    observations, infos = env.reset()\n",
    "    team_manager = TeamManager(env.agents)\n",
    "    teams = team_manager.get_teams()\n",
    "    my_team = team_manager.get_my_team()\n",
    "    opponent_team = team_manager.get_other_team()\n",
    "    \n",
    "    hidden = q.init_hidden()\n",
    "    opponent_hidden = q.init_hidden()\n",
    "    score = 0.0\n",
    "\n",
    "    while not team_manager.has_terminated_teams():\n",
    "        # Fill rows with zeros for terminated agents\n",
    "        for agent in team_manager.agents:\n",
    "            if agent not in observations or observations[agent] is None:\n",
    "                observations[agent] = np.zeros(q.n_obs, dtype=np.float32)\n",
    "                team_manager.terminate_agent(agent)\n",
    "\n",
    "        # Get observations for the current team and opponent\n",
    "        my_team_observations = team_manager.get_info_of_team(my_team, observations)\n",
    "        opponent_observations = team_manager.get_info_of_team(opponent_team, observations)\n",
    "\n",
    "        # Get actions for my team\n",
    "        obs_tensor = torch.tensor(np.array(list(my_team_observations.values()))).unsqueeze(0)\n",
    "        actions, hidden = q.sample_action(obs_tensor, hidden, epsilon)\n",
    "        my_team_actions = {\n",
    "            agent: action\n",
    "            for agent, action in zip(\n",
    "                my_team_observations.keys(), actions.squeeze(0).cpu().data.numpy().tolist()\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Get actions for the opponent team (self-play logic)\n",
    "        opponent_obs_tensor = torch.tensor(np.array(list(opponent_observations.values()))).unsqueeze(0)\n",
    "        opponent_actions, opponent_hidden = opponent_q.sample_action(opponent_obs_tensor, opponent_hidden, epsilon)\n",
    "        opponent_team_actions = {\n",
    "            agent: action\n",
    "            for agent, action in zip(\n",
    "                opponent_observations.keys(), opponent_actions.squeeze(0).cpu().data.numpy().tolist()\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Combine actions\n",
    "        agent_actions = {**my_team_actions, **opponent_team_actions}\n",
    "\n",
    "        # Terminated agents use None action\n",
    "        for agent in team_manager.terminated_agents:\n",
    "            agent_actions[agent] = None\n",
    "\n",
    "        # Step the environment\n",
    "        observations, agent_rewards, agent_terminations, agent_truncations, agent_infos = env.step(agent_actions)\n",
    "        score += sum(team_manager.get_info_of_team(my_team, agent_rewards, 0).values())\n",
    "\n",
    "        if memory is not None:\n",
    "            # Fill rows with zeros for terminated agents\n",
    "            next_observations = [\n",
    "                observations[agent]\n",
    "                if agent in observations and observations[agent] is not None\n",
    "                else np.zeros(q.n_obs, dtype=np.float32)\n",
    "                for agent in team_manager.get_my_agents()\n",
    "            ]\n",
    "            my_team_actions = [\n",
    "                agent_actions[agent]\n",
    "                if agent in agent_actions and agent_actions[agent] is not None\n",
    "                else 0\n",
    "                for agent in team_manager.get_my_agents()\n",
    "            ]\n",
    "\n",
    "            memory.put((\n",
    "                list(my_team_observations.values()),\n",
    "                my_team_actions,\n",
    "                list(team_manager.get_info_of_team(my_team, agent_rewards, 0).values()),\n",
    "                next_observations,\n",
    "                list(team_manager.get_info_of_team(\n",
    "                    my_team,\n",
    "                    TeamManager.merge_terminates_truncates(agent_terminations, agent_truncations)).values())\n",
    "            ))\n",
    "\n",
    "        # Check for termination\n",
    "        for agent, done in agent_terminations.items():\n",
    "            if done:\n",
    "                team_manager.terminate_agent(agent)\n",
    "        for agent, done in agent_truncations.items():\n",
    "            if done:\n",
    "                team_manager.terminate_agent(agent)\n",
    "\n",
    "        # Break if the other team has less than 3 agents\n",
    "        if len(team_manager.get_other_team_remains()) <= 3:\n",
    "            break\n",
    "\n",
    "    print('Score:', score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cddd2bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.872992Z",
     "iopub.status.busy": "2024-12-18T16:55:22.872677Z",
     "iopub.status.idle": "2024-12-18T16:55:22.877816Z",
     "shell.execute_reply": "2024-12-18T16:55:22.876869Z"
    },
    "papermill": {
     "duration": 0.012119,
     "end_time": "2024-12-18T16:55:22.879555",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.867436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "def evaluate_model(env, num_episodes, model1, model2, run_episode_fn):\n",
    "    \"\"\"\n",
    "\n",
    "    :param env: Environment\n",
    "    :param num_episodes: How many episodes to test\n",
    "    :param model: Trained model\n",
    "    :param run_episode_fn: function to run an episode\n",
    "    :return: average score over num_episodes\n",
    "    \"\"\"\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    score = 0\n",
    "    for episode_i in range(num_episodes):\n",
    "        score += run_episode_fn(env, model1, model2, epsilon=0)\n",
    "    return score / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfda06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.888935Z",
     "iopub.status.busy": "2024-12-18T16:55:22.888649Z",
     "iopub.status.idle": "2024-12-18T16:55:22.903537Z",
     "shell.execute_reply": "2024-12-18T16:55:22.902524Z"
    },
    "papermill": {
     "duration": 0.022097,
     "end_time": "2024-12-18T16:55:22.905793",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.883696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run\n",
    "\n",
    "def run_model_train_test(\n",
    "        env,\n",
    "        test_env,\n",
    "        model_team1,\n",
    "        model_team2,\n",
    "        target_model_team1,\n",
    "        target_model_team2,\n",
    "        save_name_team1,\n",
    "        save_name_team2,\n",
    "        team_manager,\n",
    "        hp,\n",
    "        train_fn,\n",
    "        run_episode_fn,\n",
    "        num_test_runs=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run training and testing loop of a model\n",
    "\n",
    "    :param env: Training environment\n",
    "    :param test_env: Testing environment\n",
    "    :param model: training model\n",
    "    :param target_model: target model\n",
    "    :param save_name: name to save the model\n",
    "    :param team_manager: TeamManager\n",
    "    :param hp: Hyperparameters\n",
    "    :param train_fn: training function\n",
    "    :param run_episode_fn: function to run an episode\n",
    "    :return: train_scores, test_scores\n",
    "    \"\"\"\n",
    "    reseed(seed)\n",
    "    # create env.\n",
    "    memory_team1 = ReplayBuffer(hp.buffer_limit)\n",
    "    memory_team2 = ReplayBuffer(hp.buffer_limit)\n",
    "\n",
    "    test_env.reset(seed=seed)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    target_model_team1.load_state_dict(model_team1.state_dict())\n",
    "    target_model_team2.load_state_dict(model_team2.state_dict())\n",
    "\n",
    "    # Setup env\n",
    "\n",
    "    train_scores_team1, train_scores_team2 = [], []\n",
    "    test_scores_team1, test_scores_team2 = [], []\n",
    "    losses_team1, losses_team2 = [], []\n",
    "\n",
    "    optimizer_team1 = optim.Adam(model_team1.parameters(), lr=hp.lr)\n",
    "    optimizer_team2 = optim.Adam(model_team2.parameters(), lr=hp.lr)\n",
    "\n",
    "    # Train and test\n",
    "    start_train = time.time()\n",
    "    for episode_i in range(hp.max_episodes):\n",
    "        start = time.time()\n",
    "        print(f'Episodes {episode_i + 1} / {hp.max_episodes}')\n",
    "        # Collect data\n",
    "        epsilon = max(hp.min_epsilon,\n",
    "                      hp.max_epsilon - (hp.max_epsilon - hp.min_epsilon) * (episode_i / (hp.episode_min_epsilon)))\n",
    "        \n",
    "        model_team1.eval()\n",
    "        model_team2.eval()\n",
    "        \n",
    "        train_score_team1 = run_episode_fn(env, model_team1, model_team2, memory_team1, epsilon=epsilon)\n",
    "        train_score_team2 = run_episode_fn(env, model_team2, model_team1, memory_team2, epsilon=epsilon)\n",
    "\n",
    "        train_scores_team1.append(train_score_team1)\n",
    "        train_scores_team2.append(train_score_team2)\n",
    "\n",
    "        if train_score_team1 > 200 or train_score_team2 > 200:\n",
    "            hp.min_epsilon = 0.05\n",
    "\n",
    "        # Train models\n",
    "        if memory_team1.size() > hp.warm_up_steps:\n",
    "            print(\"Training Team 1:\")\n",
    "            model_team1.train()\n",
    "            episode_losses_team1 = train_fn(\n",
    "                model_team1, target_model_team1, memory_team1, optimizer_team1,\n",
    "                hp.gamma, hp.batch_size, hp.update_iter, hp.chunk_size\n",
    "            )\n",
    "            losses_team1.append(episode_losses_team1)\n",
    "\n",
    "        if memory_team2.size() > hp.warm_up_steps:\n",
    "            print(\"Training Team 2:\")\n",
    "            model_team2.train()\n",
    "            episode_losses_team2 = train_fn(\n",
    "                model_team2, target_model_team2, memory_team2, optimizer_team2,\n",
    "                hp.gamma, hp.batch_size, hp.update_iter, hp.chunk_size\n",
    "            )\n",
    "            losses_team2.append(episode_losses_team2)\n",
    "\n",
    "        if episode_i % hp.update_target_interval == 0 and episode_i > 0:\n",
    "            target_model_team1.load_state_dict(model_team1.state_dict())\n",
    "            target_model_team2.load_state_dict(model_team2.state_dict())\n",
    "\n",
    "        # Test phase\n",
    "        if episode_i >= hp.max_episodes - 20:\n",
    "            print(\"Test phase for both teams:\")\n",
    "            model_team1.eval()\n",
    "            model_team2.eval()\n",
    "\n",
    "            avg_test_score_team1 = 0\n",
    "            avg_test_score_team2 = 0\n",
    "\n",
    "            for _ in range(num_test_runs):\n",
    "                avg_test_score_team1 += evaluate_model(test_env, hp.test_episodes, model_team1, model_team2, run_episode_fn)\n",
    "                avg_test_score_team2 += evaluate_model(test_env, hp.test_episodes, model_team2, model_team1,  run_episode_fn)\n",
    "\n",
    "            avg_test_score_team1 /= num_test_runs\n",
    "            avg_test_score_team2 /= num_test_runs\n",
    "\n",
    "            test_scores_team1.append(avg_test_score_team1)\n",
    "            test_scores_team2.append(avg_test_score_team2)\n",
    "\n",
    "            save_model(model_team1, f'vdn-{save_name_team1}-{episode_i}')\n",
    "            save_model(model_team1, f'vdn-{save_name_team2}-{episode_i}')\n",
    "\n",
    "            print(f\"Team 1 Avg Test Score: {avg_test_score_team1:.2f}\")\n",
    "            print(f\"Team 2 Avg Test Score: {avg_test_score_team2:.2f}\")\n",
    "            print('#' * 90)\n",
    "\n",
    "        print(f'Time: {time.time() - start}')\n",
    "        print(f'Total Time: {time.time() - start_train}')\n",
    "        print('-' * 90)\n",
    "\n",
    "    env.close()\n",
    "    test_env.close()\n",
    "\n",
    "    return train_scores_team1, train_scores_team2, test_scores_team1, test_scores_team2, losses_team1, losses_team2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095cc83d",
   "metadata": {
    "papermill": {
     "duration": 0.003982,
     "end_time": "2024-12-18T16:55:22.914151",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.910169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b42d9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.923752Z",
     "iopub.status.busy": "2024-12-18T16:55:22.923364Z",
     "iopub.status.idle": "2024-12-18T16:55:22.938800Z",
     "shell.execute_reply": "2024-12-18T16:55:22.937791Z"
    },
    "papermill": {
     "duration": 0.022726,
     "end_time": "2024-12-18T16:55:22.940855",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.918129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VDN\n",
    "\n",
    "class VdnQNet(nn.Module):\n",
    "\n",
    "    def __init__(self, agents, observation_spaces, action_spaces, recurrent=False):\n",
    "        super(VdnQNet, self).__init__()\n",
    "        self.agents = agents\n",
    "        self.num_agents = len(agents)\n",
    "        self.recurrent = recurrent\n",
    "        self.hx_size = 32   # latent repr size\n",
    "        self.n_obs = observation_spaces[agents[0]].shape    # observation space size of agents\n",
    "        self.n_act = action_spaces[agents[0]].n  # action space size of agents\n",
    "\n",
    "        stride1, stride2 = 1, 1\n",
    "        padding1, padding2 = 1, 1\n",
    "        kernel_size1, kernel_size2 = 3, 3\n",
    "        pool_kernel_size, pool_stride = 2, 2\n",
    "\n",
    "        height = self.n_obs[0]  # n_obs is a tuple (height, width, channels)\n",
    "        out_dim1 = compute_output_dim(height, kernel_size1, stride1, padding1) // pool_stride\n",
    "        out_dim2 = compute_output_dim(out_dim1, kernel_size2, stride2, padding2) // pool_stride\n",
    "\n",
    "        # Compute the final flattened size\n",
    "        flattened_size = out_dim2 * out_dim2 * 64\n",
    "        self.feature_cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.n_obs[2], 32, kernel_size=kernel_size1, stride=stride1, padding=padding1),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.Conv2d(32, 64, kernel_size=kernel_size2, stride=stride2, padding=padding2),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, self.hx_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if recurrent:\n",
    "            self.gru =  nn.GRUCell(self.hx_size, self.hx_size)  # shape: hx_size, hx_size\n",
    "        self.q_val = nn.Linear(self.hx_size, self.n_act)    # shape: hx_size, n_actions\n",
    "\n",
    "    def forward(self, obs, hidden):\n",
    "        \"\"\"Predict q values for each agent's actions in the batch\n",
    "        :param obs: [batch_size, num_agents, ...n_obs]\n",
    "        :param hidden: [batch_size, num_agents, hx_size]\n",
    "        :return: q_values: [batch_size, num_agents, n_actions], hidden: [batch_size, num_agents, hx_size]\n",
    "        \"\"\"\n",
    "        obs = obs.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "        \n",
    "        batch_size, num_agents, height, width, channels = obs.shape\n",
    "        obs = obs.permute(0, 1, 4, 2, 3)  # (batch_size, num_agents, channels, height, width)\n",
    "        obs = obs.reshape(batch_size * num_agents, channels, height, width)  # (batch_size * num_agents, channels, height, width)\n",
    "        \n",
    "        x = self.feature_cnn(obs)  # (batch_size * num_agents, hx_size)\n",
    "        \n",
    "        if self.recurrent:\n",
    "            hidden = hidden.reshape(batch_size * num_agents, -1)  # (batch_size * num_agents, hx_size)\n",
    "            x = self.gru(x, hidden)  # (batch_size * num_agents, hx_size)\n",
    "        \n",
    "        q_values = self.q_val(x)  # (batch_size * num_agents, n_actions)\n",
    "        \n",
    "        q_values = q_values.view(batch_size, num_agents, -1)  # (batch_size, num_agents, n_actions)\n",
    "        \n",
    "        if self.recurrent:\n",
    "            next_hidden = x.view(batch_size, num_agents, -1)  # (batch_size, num_agents, hx_size)\n",
    "        else:\n",
    "            next_hidden = hidden.view(batch_size, num_agents, -1)\n",
    "        \n",
    "        return q_values, next_hidden\n",
    "\n",
    "    def sample_action(self, obs, hidden, epsilon=1e3):\n",
    "        \"\"\"Choose action with epsilon-greedy policy, for each agent in the batch\n",
    "        :param obs: a batch of observations, [batch_size, num_agents, n_obs]\n",
    "        :param hidden: a batch of hidden states, [batch_size, num_agents, hx_size]\n",
    "        :param epsilon: exploration rate\n",
    "        :return: actions: [batch_size, num_agents], hidden: [batch_size, num_agents, hx_size]\n",
    "        \"\"\"\n",
    "        obs = obs.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "        \n",
    "        q_values, hidden = self.forward(obs, hidden)    # [batch_size, num_agents, n_actions], [batch_size, num_agents, hx_size]\n",
    "        # epsilon-greedy action selection: choose random action with epsilon probability\n",
    "        mask = (torch.rand((q_values.shape[0],), device=device) <= epsilon)  # [batch_size]\n",
    "        actions = torch.empty((q_values.shape[0], q_values.shape[1]), device=device)  # [batch_size, num_agents]\n",
    "        actions[mask] = torch.randint(0, q_values.shape[2], actions[mask].shape, device=device).float()\n",
    "        actions[~mask] = q_values[~mask].argmax(dim=2).float()  # choose action with max q value\n",
    "        return actions, hidden   # [batch_size, num_agents], [batch_size, num_agents, hx_size]\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros((batch_size, self.num_agents, self.hx_size), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf536d",
   "metadata": {
    "papermill": {
     "duration": 0.004205,
     "end_time": "2024-12-18T16:55:22.949447",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.945242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118b7a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:55:22.959923Z",
     "iopub.status.busy": "2024-12-18T16:55:22.959179Z",
     "iopub.status.idle": "2024-12-18T18:46:14.566556Z",
     "shell.execute_reply": "2024-12-18T18:46:14.565610Z"
    },
    "papermill": {
     "duration": 6651.615208,
     "end_time": "2024-12-18T18:46:14.568898",
     "exception": false,
     "start_time": "2024-12-18T16:55:22.953690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VdnHyperparameters(lr=0.002, gamma=0.99, batch_size=512, update_iter=20, buffer_limit=9000, update_target_interval=20, max_episodes=200, max_epsilon=0.9, min_epsilon=0.1, episode_min_epsilon=100, test_episodes=1, warm_up_steps=3000, chunk_size=1, recurrent=True)\n",
      "Episodes 1 / 200\n",
      "Score: -3624.1451344182715\n",
      "Score: -3543.2751333350316\n",
      "Time: 10.778191804885864\n",
      "Total Time: 10.778225660324097\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 2 / 200\n",
      "Score: -3597.900132276118\n",
      "Score: -3566.365135463886\n",
      "Time: 9.90833830833435\n",
      "Total Time: 20.68661117553711\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 3 / 200\n",
      "Score: -3668.98513507843\n",
      "Score: -3455.720130195841\n",
      "Time: 9.734773874282837\n",
      "Total Time: 30.42143416404724\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 4 / 200\n",
      "Score: -3627.135135267861\n",
      "Score: -3421.2801308939233\n",
      "Training Team 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/337114206.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_23/337114206.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.86 9.91 9.99 9.79 7.1 4.23 4.06 2.02 1.32 1.7 1.57 1.22 1.14 0.82 0.89 0.83 0.62 0.66 0.6 0.52\n",
      "Training Team 2:\n",
      "Loss: 12.31 12.38 12.04 11.99 10.44 10.65 8.4 5.96 2.46 3.33 5.31 5.6 4.04 2.55 2.18 2.69 2.48 2.14 1.6 1.96\n",
      "Time: 32.1305775642395\n",
      "Total Time: 62.552058935165405\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 5 / 200\n",
      "Score: -3838.765141826123\n",
      "Score: -2874.3001016406342\n",
      "Training Team 1:\n",
      "Loss: 1.0 0.79 1.03 1.15 0.94 0.63 0.9 1.03 0.72 0.68 0.82 0.55 0.68 0.83 0.6 0.45 0.52 0.47 0.47 0.48\n",
      "Training Team 2:\n",
      "Loss: 4.15 3.06 3.76 3.22 3.1 3.62 3.01 1.94 2.19 2.76 2.27 1.58 1.08 1.48 1.67 1.25 0.97 1.15 1.43 1.24\n",
      "Time: 31.476871967315674\n",
      "Total Time: 94.02897667884827\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 6 / 200\n",
      "Score: -2322.3150825891644\n",
      "Score: -3348.7601206768304\n",
      "Training Team 1:\n",
      "Loss: 0.77 0.71 0.72 0.56 0.58 0.65 0.61 0.43 0.71 0.63 0.34 0.55 0.68 0.47 0.26 0.32 0.25 0.24 0.23 0.14\n",
      "Training Team 2:\n",
      "Loss: 1.35 1.29 1.47 1.19 1.17 1.29 1.36 1.07 0.88 1.15 0.98 0.82 0.8 0.79 0.55 0.67 0.59 0.44 0.4 0.37\n",
      "Time: 31.347863912582397\n",
      "Total Time: 125.3769040107727\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 7 / 200\n",
      "Score: -2436.4650881784037\n",
      "Score: -3837.015141405165\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.4 0.35 0.36 0.29 0.33 0.23 0.17 0.17 0.16 0.17 0.22 0.18 0.17 0.13 0.13 0.11 0.08 0.09 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.62 0.62 0.72 0.61 0.69 0.72 0.74 0.66 0.47 0.49 0.58 0.45 0.41 0.37 0.28 0.26 0.25 0.22 0.23 0.22\n",
      "Time: 31.274303674697876\n",
      "Total Time: 156.65125274658203\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 8 / 200\n",
      "Score: -2014.470077530481\n",
      "Score: -2725.145094268024\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.23 0.15 0.15 0.17 0.13 0.14 0.17 0.15 0.1 0.12 0.13 0.08 0.09 0.1 0.15 0.09 0.09 0.11 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.36 0.32 0.39 0.28 0.32 0.35 0.23 0.16 0.11 0.13 0.2 0.16 0.19 0.18 0.13 0.13 0.11 0.14 0.1 0.1\n",
      "Time: 31.715277194976807\n",
      "Total Time: 188.3665750026703\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 9 / 200\n",
      "Score: -2996.600102018565\n",
      "Score: -2835.6300973994657\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.15 0.12 0.16 0.11 0.1 0.08 0.1 0.07 0.14 0.13 0.1 0.11 0.08 0.12 0.15 0.08 0.22 0.19 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.1 0.15 0.13 0.12 0.15 0.11 0.13 0.08 0.1 0.09 0.09 0.06 0.1 0.12 0.08 0.08 0.08 0.08 0.07\n",
      "Time: 31.353898286819458\n",
      "Total Time: 219.7205193042755\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 10 / 200\n",
      "Score: -2938.690099861473\n",
      "Score: -2628.3400889979675\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.26 0.24 0.26 0.32 0.04 0.32 0.33 0.17 0.24 0.13 0.28 0.37 0.24 0.36 0.39 0.09 0.51 0.49 0.32\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.08 0.11 0.08 0.06 0.06 0.06 0.04 0.05 0.08 0.09 0.09 0.08 0.04 0.06 0.05 0.03 0.09 0.06 0.05\n",
      "Time: 31.078519582748413\n",
      "Total Time: 250.79908299446106\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 11 / 200\n",
      "Score: -2176.240077247843\n",
      "Score: -2880.300097488798\n",
      "Training Team 1:\n",
      "Loss: 0.38 0.37 0.35 0.37 0.91 0.93 0.38 0.62 0.96 0.36 0.6 1.04 0.44 0.34 0.64 0.21 0.57 0.88 0.36 0.33\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.07 0.04 0.06 0.04 0.05 0.06 0.04 0.07 0.05 0.04 0.03 0.05 0.03 0.08 0.05 0.07 0.05 0.04 0.04\n",
      "Time: 31.06812047958374\n",
      "Total Time: 281.8672471046448\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 12 / 200\n",
      "Score: -2854.5000958917663\n",
      "Score: -2028.1750729801133\n",
      "Training Team 1:\n",
      "Loss: 0.63 0.67 0.64 0.64 0.65 0.25 0.32 0.51 0.13 0.51 0.86 0.51 0.11 0.17 0.06 0.24 0.26 0.36 0.48 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.09 0.05 0.04 0.06 0.07 0.07 0.06 0.03 0.04 0.09 0.07 0.04 0.04 0.04 0.06 0.09 0.05 0.04 0.06\n",
      "Time: 31.36321234703064\n",
      "Total Time: 313.23049569129944\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 13 / 200\n",
      "Score: -2698.04509812966\n",
      "Score: -2169.39008541964\n",
      "Training Team 1:\n",
      "Loss: 0.46 0.46 0.4 0.45 0.44 0.46 0.21 0.53 0.53 0.17 0.6 0.92 0.68 0.22 0.62 0.87 0.46 0.11 0.34 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.08 0.1 0.09 0.09 0.08 0.13 0.1 0.09 0.08 0.07 0.12 0.06 0.1 0.14 0.07 0.12 0.1 0.08 0.1\n",
      "Time: 31.22398328781128\n",
      "Total Time: 344.45452308654785\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 14 / 200\n",
      "Score: -2684.3300912259147\n",
      "Score: -2889.4000976383686\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.26 0.24 0.29 0.6 0.58 0.36 0.07 0.16 0.07 0.06 0.15 0.15 0.23 0.25 0.1 0.09 0.13 0.11 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.08 0.1 0.11 0.11 0.13 0.16 0.07 0.14 0.22 0.24 0.04 0.44 0.46 0.17 0.8 1.09 0.25 0.93 1.41\n",
      "Time: 31.660022735595703\n",
      "Total Time: 376.1145911216736\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 15 / 200\n",
      "Score: -2276.260091010481\n",
      "Score: -2504.0800915053114\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.23 0.19 0.2 0.15 0.21 0.2 0.15 0.19 0.17 0.1 0.08 0.16 0.12 0.12 0.09 0.11 0.14 0.14 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.91 0.9 0.81 0.92 0.82 0.12 0.27 0.06 0.12 0.27 0.15 0.4 0.57 0.1 0.9 1.33 1.03 0.13 1.03 1.99\n",
      "Time: 31.886892557144165\n",
      "Total Time: 408.0015285015106\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 16 / 200\n",
      "Score: -2785.80009508878\n",
      "Score: -2663.585091873072\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.13 0.13 0.14 0.11 0.08 0.11 0.1 0.08 0.14 0.08 0.09 0.12 0.13 0.12 0.1 0.11 0.07 0.08 0.13\n",
      "Training Team 2:\n",
      "Loss: 1.95 1.98 1.95 1.95 1.97 0.94 0.14 0.49 0.33 0.15 0.14 0.11 0.07 0.33 0.31 0.1 0.22 0.12 0.37 0.46\n",
      "Time: 32.158872842788696\n",
      "Total Time: 440.1604378223419\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 17 / 200\n",
      "Score: -2731.190094234422\n",
      "Score: -2691.675089802593\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.13 0.1 0.09 0.06 0.07 0.06 0.1 0.06 0.07 0.08 0.13 0.12 0.09 0.1 0.06 0.13 0.06 0.04 0.06\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.13 0.12 0.17 0.09 0.24 0.25 0.1 0.27 0.51 0.46 0.47 0.08 0.59 1.04 0.82 0.28 0.44 1.0 0.93\n",
      "Time: 32.31897473335266\n",
      "Total Time: 472.47945070266724\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 18 / 200\n",
      "Score: -1874.4500705059618\n",
      "Score: -1370.7100489847362\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.09 0.09 0.09 0.13 0.08 0.13 0.08 0.09 0.11 0.11 0.08 0.08 0.08 0.09 0.14 0.13 0.14 0.09 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.28 0.26 0.31 0.3 0.13 0.44 0.41 0.36 0.09 0.65 1.07 0.85 0.34 0.39 0.86 0.8 0.43 0.13 0.26 0.19\n",
      "Time: 31.851162910461426\n",
      "Total Time: 504.33065962791443\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 19 / 200\n",
      "Score: -2109.3500759806484\n",
      "Score: -2461.895087843761\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.12 0.11 0.15 0.07 0.08 0.09 0.12 0.1 0.14 0.18 0.16 0.09 0.08 0.13 0.07 0.11 0.1 0.1 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.21 0.17 0.22 0.2 0.31 0.33 0.22 0.16 0.19 0.18 0.15 0.16 0.24 0.23 0.16 0.16 0.24 0.2 0.11 0.17\n",
      "Time: 31.653290510177612\n",
      "Total Time: 535.9839906692505\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 20 / 200\n",
      "Score: -2100.730076080188\n",
      "Score: -2346.680083184503\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.1 0.09 0.11 0.09 0.15 0.11 0.13 0.08 0.11 0.14 0.1 0.06 0.13 0.19 0.21 0.1 0.14 0.26 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.2 0.21 0.19 0.07 0.11 0.2 0.13 0.17 0.14 0.11 0.1 0.2 0.11 0.08 0.12 0.09 0.2 0.21 0.13\n",
      "Time: 31.329824924468994\n",
      "Total Time: 567.3138625621796\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 21 / 200\n",
      "Score: -2618.4000868108124\n",
      "Score: -2637.22508809343\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.12 0.29 0.32 0.27 0.26 0.1 0.25 0.33 0.36 0.12 0.35 0.55 0.54 0.23 0.26 0.48 0.18 0.32 0.53\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.11 0.12 0.15 0.2 0.2 0.12 0.15 0.17 0.12 0.09 0.15 0.13 0.11 0.1 0.11 0.07 0.1 0.08 0.13\n",
      "Time: 31.07301640510559\n",
      "Total Time: 598.3869562149048\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 22 / 200\n",
      "Score: -2597.900087513961\n",
      "Score: -2611.595086472109\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.24 0.22 0.23 0.29 0.27 0.07 0.27 0.44 0.46 0.16 0.48 0.76 0.46 0.13 0.24 0.12 0.57 0.74 0.36\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.13 0.14 0.14 0.13 0.11 0.14 0.13 0.13 0.13 0.06 0.1 0.05 0.06 0.08 0.12 0.07 0.08 0.08 0.09\n",
      "Time: 31.309634685516357\n",
      "Total Time: 629.696652173996\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 23 / 200\n",
      "Score: -2089.4550765817985\n",
      "Score: -2432.3550795624033\n",
      "Training Team 1:\n",
      "Loss: 0.43 0.45 0.5 0.47 0.93 0.95 0.96 0.43 0.69 1.08 0.86 0.29 0.86 1.61 1.42 0.5 0.31 0.71 0.65 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.08 0.1 0.07 0.07 0.07 0.05 0.1 0.08 0.09 0.06 0.05 0.06 0.05 0.09 0.08 0.1 0.04 0.06 0.04\n",
      "Time: 31.204135179519653\n",
      "Total Time: 660.9008302688599\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 24 / 200\n",
      "Score: -2697.3000895874575\n",
      "Score: -2492.900081981905\n",
      "Training Team 1:\n",
      "Loss: 0.68 0.65 0.61 0.62 0.69 1.15 0.98 0.29 0.41 0.92 0.79 0.31 0.28 0.65 0.42 0.13 0.48 0.74 0.5 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.07 0.06 0.04 0.05 0.05 0.05 0.05 0.09 0.05 0.07 0.09 0.04 0.03 0.07 0.05 0.04 0.04 0.04 0.05\n",
      "Time: 31.202221393585205\n",
      "Total Time: 692.1030876636505\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 25 / 200\n",
      "Score: -2524.3000821480528\n",
      "Score: -2481.9000813858584\n",
      "Training Team 1:\n",
      "Loss: 0.57 0.53 0.51 0.53 0.5 0.63 0.3 0.14 0.31 0.15 0.28 0.31 0.15 0.32 0.51 0.27 0.1 0.17 0.09 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.07 0.05 0.03 0.05 0.05 0.02 0.06 0.03 0.06 0.05 0.03 0.06 0.03 0.04 0.05 0.05 0.07 0.03 0.02\n",
      "Time: 31.717419624328613\n",
      "Total Time: 723.8205506801605\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 26 / 200\n",
      "Score: -2518.90008297842\n",
      "Score: -2530.1300833765417\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.1 0.14 0.07 0.15 0.12 0.13 0.1 0.11 0.08 0.06 0.09 0.12 0.11 0.1 0.1 0.14 0.08 0.08 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.06 0.05 0.03 0.05 0.02 0.03 0.03 0.04 0.02 0.05 0.03 0.05 0.04 0.05 0.04 0.02 0.05 0.04 0.03\n",
      "Time: 31.361799240112305\n",
      "Total Time: 755.1823947429657\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 27 / 200\n",
      "Score: -2138.7000727681443\n",
      "Score: -2269.5150782819837\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.06 0.1 0.08 0.05 0.1 0.08 0.06 0.08 0.04 0.09 0.1 0.05 0.07 0.18 0.11 0.08 0.07 0.11 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.06 0.02 0.03 0.03 0.06 0.04 0.03 0.07 0.06 0.07 0.04 0.05 0.03 0.03 0.03 0.02 0.02 0.04 0.05\n",
      "Time: 31.700385808944702\n",
      "Total Time: 786.8828229904175\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 28 / 200\n",
      "Score: -2500.2000814005733\n",
      "Score: -2353.010078904219\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.09 0.06 0.06 0.05 0.06 0.06 0.08 0.07 0.04 0.04 0.1 0.05 0.09 0.07 0.03 0.09 0.08 0.07 0.03\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.05 0.03 0.04 0.03 0.06 0.03 0.04 0.04 0.03 0.04 0.03 0.04 0.04 0.04 0.05 0.04 0.05 0.05 0.06\n",
      "Time: 31.6492178440094\n",
      "Total Time: 818.5320842266083\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 29 / 200\n",
      "Score: -2216.3000769326463\n",
      "Score: -2096.140068925917\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.1 0.09 0.05 0.06 0.05 0.09 0.07 0.05 0.05 0.03 0.07 0.08 0.09 0.05 0.04 0.05 0.08 0.05 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.04 0.02 0.02 0.03 0.04 0.03 0.02 0.02 0.03 0.02 0.02 0.03 0.02 0.03 0.03 0.03 0.01 0.03 0.03\n",
      "Time: 32.34099102020264\n",
      "Total Time: 850.8731331825256\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 30 / 200\n",
      "Score: -1640.3250604355708\n",
      "Score: -2200.800075020641\n",
      "Training Team 1:\n",
      "Loss: 0.04 0.06 0.07 0.07 0.08 0.06 0.08 0.05 0.03 0.04 0.07 0.06 0.09 0.05 0.08 0.04 0.06 0.06 0.05 0.04\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.06 0.02 0.04 0.02 0.03 0.03 0.05 0.05 0.02 0.02 0.04 0.03 0.07 0.05 0.06 0.08 0.02 0.06 0.05\n",
      "Time: 32.13055682182312\n",
      "Total Time: 883.0037426948547\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 31 / 200\n",
      "Score: -2098.1000731484964\n",
      "Score: -1995.4350668517873\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.06 0.12 0.08 0.08 0.05 0.09 0.07 0.08 0.12 0.13 0.08 0.11 0.07 0.08 0.07 0.08 0.05 0.09 0.06\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.05 0.03 0.04 0.04 0.05 0.03 0.06 0.07 0.05 0.07 0.04 0.04 0.05 0.06 0.05 0.03 0.03 0.04 0.02\n",
      "Time: 31.981544256210327\n",
      "Total Time: 914.9853293895721\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 32 / 200\n",
      "Score: -1777.9500670023263\n",
      "Score: -1442.0600519115105\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.06 0.12 0.08 0.07 0.11 0.13 0.08 0.07 0.13 0.13 0.08 0.07 0.07 0.07 0.08 0.14 0.06 0.05 0.07\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.06 0.05 0.08 0.04 0.07 0.11 0.07 0.04 0.06 0.06 0.07 0.07 0.06 0.1 0.06 0.07 0.08 0.09 0.09\n",
      "Time: 31.394875288009644\n",
      "Total Time: 946.3802423477173\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 33 / 200\n",
      "Score: -1787.0200616475195\n",
      "Score: -2031.5700715724379\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.09 0.09 0.08 0.07 0.08 0.07 0.08 0.1 0.11 0.08 0.09 0.08 0.13 0.1 0.1 0.12 0.12 0.1 0.08\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.05 0.07 0.13 0.11 0.07 0.12 0.21 0.18 0.06 0.16 0.26 0.11 0.1 0.11 0.09 0.1 0.04 0.18 0.15\n",
      "Time: 31.674134254455566\n",
      "Total Time: 978.0544185638428\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 34 / 200\n",
      "Score: -1655.3450636574998\n",
      "Score: -1575.3300612838939\n",
      "Training Team 1:\n",
      "Loss: 0.08 0.08 0.1 0.11 0.09 0.1 0.08 0.1 0.11 0.11 0.1 0.09 0.12 0.08 0.11 0.07 0.11 0.11 0.07 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.11 0.17 0.18 0.16 0.13 0.11 0.14 0.08 0.12 0.11 0.06 0.14 0.14 0.16 0.14 0.12 0.09 0.11 0.08\n",
      "Time: 31.791120290756226\n",
      "Total Time: 1009.8455903530121\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 35 / 200\n",
      "Score: -2156.6400681594387\n",
      "Score: -1376.4650522433221\n",
      "Training Team 1:\n",
      "Loss: 0.08 0.16 0.14 0.1 0.15 0.08 0.13 0.09 0.09 0.07 0.08 0.11 0.14 0.1 0.1 0.13 0.12 0.1 0.15 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.13 0.18 0.15 0.18 0.18 0.17 0.14 0.14 0.14 0.12 0.15 0.12 0.15 0.13 0.15 0.11 0.32 0.29 0.23 0.18\n",
      "Time: 31.805652618408203\n",
      "Total Time: 1041.6512846946716\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 36 / 200\n",
      "Score: -1893.3400631062686\n",
      "Score: -1541.6550556719303\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.26 0.2 0.12 0.07 0.21 0.23 0.1 0.23 0.24 0.2 0.1 0.09 0.08 0.14 0.17 0.09 0.13 0.15 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.23 0.25 0.17 0.16 0.25 0.45 0.36 0.1 0.64 1.0 0.74 0.18 1.04 1.87 1.81 0.96 0.09 1.15 1.99\n",
      "Time: 31.59976363182068\n",
      "Total Time: 1073.251098871231\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 37 / 200\n",
      "Score: -1082.4900481915101\n",
      "Score: -944.4600419299677\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.13 0.2 0.17 0.18 0.21 0.1 0.25 0.24 0.25 0.17 0.17 0.19 0.13 0.3 0.25 0.18 0.19 0.22 0.18\n",
      "Training Team 2:\n",
      "Loss: 2.24 2.15 2.21 2.16 2.18 2.14 1.32 0.45 0.91 0.3 0.55 0.68 0.15 0.33 0.28 0.2 0.33 0.2 0.66 0.74\n",
      "Time: 31.45626187324524\n",
      "Total Time: 1104.7073981761932\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 38 / 200\n",
      "Score: -2226.900068999268\n",
      "Score: -2160.2000696184114\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.17 0.22 0.2 0.27 0.32 0.16 0.37 0.55 0.23 0.26 0.42 0.26 0.15 0.27 0.11 0.39 0.53 0.33 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.17 0.2 0.26 0.37 0.43 0.4 0.67 0.49 0.22 0.34 0.64 0.4 0.21 0.42 0.63 0.47 0.13 0.42 0.75 0.76\n",
      "Time: 31.27604365348816\n",
      "Total Time: 1135.983478307724\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 39 / 200\n",
      "Score: -1845.820063907653\n",
      "Score: -1730.0550589570776\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.2 0.17 0.19 0.14 0.15 0.11 0.13 0.14 0.12 0.16 0.25 0.14 0.2 0.17 0.12 0.17 0.21 0.18 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.38 0.37 0.33 0.45 0.4 0.15 0.37 0.45 0.27 0.18 0.28 0.28 0.13 0.36 0.58 0.45 0.16 0.34 0.52 0.42\n",
      "Time: 31.470563650131226\n",
      "Total Time: 1167.4540839195251\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 40 / 200\n",
      "Score: -2188.1000675214455\n",
      "Score: -2126.81006698031\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.14 0.19 0.15 0.15 0.1 0.11 0.1 0.09 0.17 0.11 0.13 0.19 0.16 0.12 0.11 0.07 0.17 0.23 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.17 0.14 0.23 0.2 0.24 0.23 0.36 0.21 0.13 0.17 0.16 0.12 0.1 0.14 0.15 0.14 0.15 0.09 0.12 0.12\n",
      "Time: 31.548468589782715\n",
      "Total Time: 1199.0025901794434\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 41 / 200\n",
      "Score: -1924.0000601159409\n",
      "Score: -2181.39007077273\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.12 0.27 0.26 0.21 0.26 0.1 0.13 0.23 0.13 0.08 0.21 0.33 0.28 0.13 0.24 0.38 0.22 0.06 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.14 0.2 0.18 0.25 0.27 0.21 0.2 0.18 0.18 0.18 0.13 0.17 0.2 0.17 0.11 0.17 0.11 0.11 0.16\n",
      "Time: 31.545222759246826\n",
      "Total Time: 1230.5478601455688\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 42 / 200\n",
      "Score: -2131.800067115575\n",
      "Score: -1499.6900518648326\n",
      "Training Team 1:\n",
      "Loss: 0.06 0.07 0.17 0.17 0.15 0.14 0.11 0.11 0.11 0.12 0.11 0.18 0.09 0.09 0.11 0.07 0.16 0.15 0.07 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.2 0.12 0.07 0.11 0.11 0.11 0.12 0.12 0.12 0.17 0.12 0.16 0.16 0.18 0.07 0.14 0.12 0.11 0.09 0.14\n",
      "Time: 32.01880502700806\n",
      "Total Time: 1262.566701889038\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 43 / 200\n",
      "Score: -2044.0200654668733\n",
      "Score: -2084.945065601729\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.11 0.11 0.07 0.04 0.1 0.18 0.11 0.05 0.13 0.1 0.13 0.13 0.1 0.09 0.1 0.07 0.07 0.11 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.14 0.15 0.12 0.11 0.1 0.06 0.09 0.08 0.1 0.11 0.12 0.08 0.06 0.08 0.09 0.09 0.07 0.08 0.06\n",
      "Time: 32.115543365478516\n",
      "Total Time: 1294.682285785675\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 44 / 200\n",
      "Score: -2149.0000657066703\n",
      "Score: -2183.2000672891736\n",
      "Training Team 1:\n",
      "Loss: 0.08 0.05 0.07 0.13 0.12 0.1 0.04 0.11 0.2 0.12 0.13 0.09 0.09 0.11 0.1 0.05 0.12 0.12 0.15 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.08 0.09 0.07 0.11 0.07 0.1 0.07 0.12 0.08 0.08 0.11 0.07 0.11 0.09 0.07 0.12 0.1 0.12 0.07\n",
      "Time: 32.15733504295349\n",
      "Total Time: 1326.8396668434143\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 45 / 200\n",
      "Score: -2040.2700634589419\n",
      "Score: -1927.8000608226284\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.16 0.19 0.12 0.11 0.04 0.08 0.05 0.05 0.1 0.04 0.05 0.03 0.08 0.12 0.05 0.11 0.16 0.07 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.06 0.06 0.06 0.07 0.05 0.06 0.12 0.07 0.08 0.04 0.09 0.05 0.09 0.09 0.06 0.04 0.06 0.08 0.06\n",
      "Time: 31.84067130088806\n",
      "Total Time: 1358.6803753376007\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 46 / 200\n",
      "Score: -2082.1000634944066\n",
      "Score: -1848.455058480613\n",
      "Training Team 1:\n",
      "Loss: 0.05 0.03 0.05 0.03 0.01 0.04 0.04 0.03 0.03 0.03 0.05 0.03 0.04 0.02 0.04 0.04 0.03 0.03 0.04 0.02\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.05 0.03 0.03 0.05 0.03 0.06 0.08 0.04 0.03 0.06 0.05 0.06 0.05 0.03 0.07 0.03 0.05 0.06 0.04\n",
      "Time: 31.603814363479614\n",
      "Total Time: 1390.2842297554016\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 47 / 200\n",
      "Score: -1979.0600601974875\n",
      "Score: -1541.5000525098294\n",
      "Training Team 1:\n",
      "Loss: 0.03 0.04 0.02 0.03 0.03 0.02 0.03 0.02 0.03 0.06 0.04 0.02 0.06 0.03 0.03 0.03 0.05 0.03 0.05 0.03\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.04 0.03 0.03 0.06 0.07 0.05 0.07 0.04 0.07 0.07 0.07 0.04 0.06 0.06 0.08 0.06 0.03 0.05 0.08\n",
      "Time: 31.340171337127686\n",
      "Total Time: 1421.6244447231293\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 48 / 200\n",
      "Score: -1164.085046258755\n",
      "Score: -1566.6950525157154\n",
      "Training Team 1:\n",
      "Loss: 0.05 0.07 0.04 0.04 0.06 0.06 0.05 0.03 0.06 0.02 0.04 0.04 0.06 0.02 0.05 0.07 0.02 0.06 0.03 0.04\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.04 0.11 0.05 0.04 0.05 0.11 0.04 0.09 0.09 0.08 0.06 0.07 0.1 0.06 0.03 0.06 0.04 0.07 0.06\n",
      "Time: 31.12295889854431\n",
      "Total Time: 1452.747441291809\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 49 / 200\n",
      "Score: -1756.3500532852486\n",
      "Score: -1852.3300570817664\n",
      "Training Team 1:\n",
      "Loss: 0.05 0.04 0.06 0.05 0.04 0.05 0.07 0.06 0.01 0.05 0.06 0.03 0.04 0.05 0.03 0.04 0.07 0.05 0.08 0.06\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.05 0.03 0.05 0.05 0.06 0.07 0.09 0.1 0.08 0.07 0.06 0.06 0.06 0.05 0.04 0.09 0.11 0.06 0.02\n",
      "Time: 30.83264446258545\n",
      "Total Time: 1483.58012008667\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 50 / 200\n",
      "Score: -469.18002982530743\n",
      "Score: -841.4250388043001\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.1 0.06 0.06 0.1 0.05 0.08 0.06 0.07 0.06 0.08 0.07 0.07 0.03 0.07 0.06 0.08 0.05 0.13 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.09 0.07 0.11 0.07 0.08 0.06 0.09 0.11 0.08 0.03 0.1 0.08 0.08 0.1 0.06 0.09 0.07 0.08 0.1\n",
      "Time: 29.27224850654602\n",
      "Total Time: 1512.8524107933044\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 51 / 200\n",
      "Score: -1841.6000537723303\n",
      "Score: -1825.1650533126667\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.06 0.07 0.07 0.08 0.09 0.08 0.12 0.06 0.1 0.08 0.09 0.07 0.07 0.12 0.06 0.05 0.07 0.08 0.08\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.09 0.08 0.05 0.06 0.07 0.06 0.09 0.07 0.05 0.07 0.08 0.06 0.07 0.11 0.07 0.07 0.06 0.09 0.06\n",
      "Time: 30.911999702453613\n",
      "Total Time: 1543.7644519805908\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 52 / 200\n",
      "Score: -1522.1350499819964\n",
      "Score: -1583.9650509962812\n",
      "Training Team 1:\n",
      "Loss: 0.09 0.11 0.1 0.11 0.11 0.06 0.04 0.09 0.06 0.08 0.06 0.08 0.08 0.08 0.06 0.15 0.11 0.1 0.06 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.12 0.1 0.08 0.06 0.05 0.06 0.09 0.1 0.07 0.06 0.08 0.08 0.07 0.1 0.05 0.05 0.08 0.09 0.12\n",
      "Time: 30.706910848617554\n",
      "Total Time: 1574.4714069366455\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 53 / 200\n",
      "Score: -1496.7600517394021\n",
      "Score: -1583.565050506033\n",
      "Training Team 1:\n",
      "Loss: 0.09 0.06 0.1 0.06 0.14 0.12 0.09 0.1 0.09 0.1 0.11 0.08 0.15 0.08 0.13 0.11 0.09 0.11 0.07 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.08 0.11 0.09 0.06 0.05 0.08 0.1 0.11 0.09 0.08 0.1 0.08 0.08 0.07 0.1 0.07 0.11 0.06 0.07\n",
      "Time: 30.493566274642944\n",
      "Total Time: 1604.9650197029114\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 54 / 200\n",
      "Score: -1858.8150548869744\n",
      "Score: -1855.2000537570566\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.1 0.07 0.09 0.09 0.11 0.12 0.06 0.06 0.08 0.07 0.09 0.03 0.07 0.12 0.1 0.1 0.14 0.05 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.07 0.08 0.09 0.06 0.06 0.1 0.08 0.06 0.1 0.06 0.12 0.09 0.08 0.07 0.06 0.05 0.09 0.06 0.05\n",
      "Time: 30.340206146240234\n",
      "Total Time: 1635.3052611351013\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 55 / 200\n",
      "Score: -1810.5000516632572\n",
      "Score: -1939.1000568540767\n",
      "Training Team 1:\n",
      "Loss: 0.08 0.11 0.05 0.17 0.1 0.08 0.08 0.08 0.09 0.07 0.05 0.09 0.08 0.04 0.08 0.07 0.1 0.06 0.07 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.1 0.07 0.1 0.07 0.06 0.08 0.09 0.07 0.08 0.07 0.08 0.06 0.13 0.1 0.1 0.08 0.06 0.1 0.12\n",
      "Time: 30.586498975753784\n",
      "Total Time: 1665.891807794571\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 56 / 200\n",
      "Score: -1819.8000521007925\n",
      "Score: -1849.9000530885532\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.06 0.06 0.09 0.1 0.07 0.08 0.18 0.09 0.07 0.06 0.12 0.05 0.06 0.09 0.12 0.07 0.13 0.07 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.05 0.09 0.11 0.1 0.06 0.05 0.06 0.06 0.04 0.05 0.06 0.08 0.07 0.1 0.09 0.08 0.09 0.06 0.12\n",
      "Time: 30.87705636024475\n",
      "Total Time: 1696.7689046859741\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 57 / 200\n",
      "Score: -1354.40005083289\n",
      "Score: -655.840027930215\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.07 0.12 0.09 0.14 0.12 0.05 0.1 0.16 0.08 0.09 0.12 0.07 0.13 0.1 0.1 0.11 0.11 0.11 0.06\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.09 0.12 0.07 0.09 0.07 0.1 0.09 0.09 0.05 0.09 0.07 0.05 0.09 0.08 0.07 0.06 0.03 0.04 0.08\n",
      "Time: 30.73528242111206\n",
      "Total Time: 1727.504224061966\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 58 / 200\n",
      "Score: -1756.200050091371\n",
      "Score: -1766.1500507798046\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.12 0.09 0.14 0.11 0.09 0.06 0.05 0.08 0.11 0.06 0.08 0.07 0.05 0.12 0.08 0.08 0.09 0.09 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.13 0.05 0.04 0.07 0.06 0.05 0.08 0.09 0.05 0.07 0.09 0.11 0.06 0.03 0.07 0.09 0.07 0.09 0.05\n",
      "Time: 30.965020656585693\n",
      "Total Time: 1758.4692919254303\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 59 / 200\n",
      "Score: -1604.3250478878617\n",
      "Score: -1585.7500459235162\n",
      "Training Team 1:\n",
      "Loss: 0.05 0.04 0.04 0.05 0.03 0.06 0.09 0.04 0.05 0.03 0.05 0.04 0.04 0.01 0.06 0.07 0.04 0.07 0.03 0.07\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.04 0.04 0.07 0.04 0.06 0.03 0.03 0.03 0.06 0.03 0.06 0.03 0.04 0.03 0.05 0.04 0.05 0.03 0.06\n",
      "Time: 30.99601173400879\n",
      "Total Time: 1789.465342283249\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 60 / 200\n",
      "Score: -1813.3750524921343\n",
      "Score: -1661.6200461424887\n",
      "Training Team 1:\n",
      "Loss: 0.06 0.04 0.06 0.02 0.06 0.03 0.07 0.06 0.03 0.03 0.05 0.04 0.04 0.08 0.03 0.06 0.08 0.04 0.04 0.06\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.1 0.06 0.06 0.06 0.04 0.05 0.06 0.05 0.07 0.04 0.1 0.07 0.04 0.04 0.05 0.06 0.03 0.08 0.05\n",
      "Time: 31.07502555847168\n",
      "Total Time: 1820.5404098033905\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 61 / 200\n",
      "Score: -1495.780041584745\n",
      "Score: -1537.3400423750281\n",
      "Training Team 1:\n",
      "Loss: 0.09 0.03 0.02 0.03 0.05 0.05 0.05 0.03 0.08 0.01 0.05 0.05 0.04 0.09 0.06 0.09 0.07 0.04 0.05 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.07 0.04 0.05 0.05 0.02 0.05 0.05 0.04 0.05 0.06 0.09 0.11 0.09 0.05 0.03 0.06 0.02 0.07 0.03\n",
      "Time: 31.613999843597412\n",
      "Total Time: 1852.1544561386108\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 62 / 200\n",
      "Score: -1019.5050423927605\n",
      "Score: -353.5400130916387\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.16 0.13 0.08 0.09 0.11 0.1 0.09 0.1 0.09 0.06 0.12 0.13 0.12 0.05 0.1 0.08 0.09 0.09 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.06 0.05 0.11 0.09 0.07 0.09 0.06 0.06 0.07 0.02 0.03 0.03 0.06 0.03 0.04 0.05 0.06 0.08 0.05\n",
      "Time: 31.906606197357178\n",
      "Total Time: 1884.0611057281494\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 63 / 200\n",
      "Score: -1545.7000404084101\n",
      "Score: -1647.5000441772863\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.11 0.13 0.21 0.03 0.12 0.13 0.07 0.11 0.13 0.04 0.11 0.11 0.08 0.11 0.07 0.11 0.14 0.1 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.03 0.06 0.07 0.09 0.04 0.04 0.03 0.05 0.06 0.06 0.07 0.03 0.04 0.07 0.03 0.04 0.04 0.06 0.05\n",
      "Time: 31.613276481628418\n",
      "Total Time: 1915.6744287014008\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 64 / 200\n",
      "Score: -1628.5000433633104\n",
      "Score: -1608.5750428587198\n",
      "Training Team 1:\n",
      "Loss: 0.36 0.39 0.37 0.36 0.33 0.16 0.09 0.16 0.06 0.22 0.23 0.09 0.18 0.24 0.17 0.18 0.15 0.09 0.17 0.23\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.13 0.05 0.03 0.04 0.02 0.06 0.03 0.04 0.06 0.11 0.13 0.06 0.15 0.21 0.23 0.12 0.07 0.57 0.6\n",
      "Time: 31.95956778526306\n",
      "Total Time: 1947.634045600891\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 65 / 200\n",
      "Score: -1622.6000431105494\n",
      "Score: -1552.3000404229388\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.18 0.22 0.2 0.28 0.25 0.12 0.28 0.23 0.14 0.16 0.14 0.13 0.21 0.19 0.15 0.1 0.11 0.14 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.06 0.35 0.31 0.35 0.35 0.38 0.25 0.07 0.09 0.21 0.19 0.13 0.11 0.13 0.12 0.14 0.08 0.2 0.21\n",
      "Time: 32.25265645980835\n",
      "Total Time: 1979.8867492675781\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 66 / 200\n",
      "Score: -1534.6000393405557\n",
      "Score: -1469.800036681816\n",
      "Training Team 1:\n",
      "Loss: 0.21 0.2 0.21 0.22 0.22 0.08 0.13 0.17 0.12 0.17 0.15 0.04 0.08 0.06 0.1 0.05 0.23 0.24 0.07 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.05 0.05 0.07 0.31 0.32 0.31 0.18 0.11 0.16 0.06 0.02 0.09 0.07 0.03 0.06 0.09 0.06 0.13 0.12\n",
      "Time: 32.79685640335083\n",
      "Total Time: 2012.6836540699005\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 67 / 200\n",
      "Score: -1535.700040393509\n",
      "Score: -1362.895036507398\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.23 0.19 0.21 0.2 0.11 0.14 0.07 0.05 0.13 0.09 0.23 0.23 0.08 0.88 0.91 0.11 1.11 1.49 0.93\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.15 0.16 0.16 0.13 0.34 0.35 0.24 0.11 0.18 0.13 0.04 0.1 0.17 0.1 0.07 0.09 0.04 0.07 0.07\n",
      "Time: 34.877150774002075\n",
      "Total Time: 2047.5608503818512\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 68 / 200\n",
      "Score: -1522.9200409343466\n",
      "Score: -1505.5000398652628\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.2 0.17 0.21 1.06 1.04 1.02 0.18 1.92 1.68 0.4 0.43 0.43 0.3 0.59 0.38 0.31 0.43 0.19 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.07 0.09 0.06 0.1 0.1 0.08 0.07 0.09 0.04 0.02 0.03 0.03 0.03 0.02 0.02 0.05 0.03 0.03 0.02\n",
      "Time: 34.199880599975586\n",
      "Total Time: 2081.760775089264\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 69 / 200\n",
      "Score: -1317.135034834966\n",
      "Score: -187.9250162318349\n",
      "Training Team 1:\n",
      "Loss: 0.48 0.44 0.47 0.48 0.5 0.51 0.56 0.25 0.31 0.33 0.51 0.38 0.15 0.16 0.17 0.13 0.21 0.24 0.11 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.06 0.04 0.03 0.04 0.06 0.05 0.04 0.07 0.05 0.09 0.03 0.05 0.06 0.06 0.03 0.07 0.07 0.04 0.06\n",
      "Time: 33.66451549530029\n",
      "Total Time: 2115.42533493042\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 70 / 200\n",
      "Score: -1108.7150312466547\n",
      "Score: -875.4900274956599\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.26 0.22 0.26 0.23 0.29 0.22 0.16 0.13 0.12 0.2 0.19 0.15 0.09 0.11 0.17 0.08 0.1 0.1 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.11 0.14 0.1 0.05 0.08 0.09 0.04 0.08 0.04 0.09 0.06 0.05 0.04 0.09 0.07 0.06 0.07 0.03 0.04\n",
      "Time: 33.20129656791687\n",
      "Total Time: 2148.6266765594482\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 71 / 200\n",
      "Score: -1455.3000359432772\n",
      "Score: -1386.4000330027193\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.08 0.09 0.07 0.11 0.12 0.09 0.05 0.04 0.04 0.04 0.04 0.05 0.05 0.05 0.03 0.03 0.04 0.02 0.04\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.04 0.06 0.06 0.04 0.05 0.02 0.06 0.06 0.05 0.03 0.03 0.05 0.03 0.05 0.08 0.05 0.07 0.03 0.05\n",
      "Time: 33.33336901664734\n",
      "Total Time: 2181.9600853919983\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 72 / 200\n",
      "Score: -1407.9300358509645\n",
      "Score: -1384.2000329084694\n",
      "Training Team 1:\n",
      "Loss: 0.04 0.05 0.04 0.03 0.04 0.02 0.01 0.03 0.01 0.02 0.01 0.01 0.0 0.01 0.02 0.03 0.03 0.02 0.02 0.02\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.05 0.06 0.05 0.07 0.06 0.08 0.07 0.06 0.04 0.06 0.06 0.08 0.05 0.05 0.06 0.06 0.09 0.04 0.07\n",
      "Time: 32.44319295883179\n",
      "Total Time: 2214.4033238887787\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 73 / 200\n",
      "Score: -946.6250357367098\n",
      "Score: -259.2000110298395\n",
      "Training Team 1:\n",
      "Loss: 0.06 0.1 0.04 0.07 0.05 0.07 0.04 0.03 0.04 0.09 0.07 0.04 0.08 0.07 0.11 0.1 0.03 0.11 0.03 0.05\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.12 0.04 0.07 0.06 0.07 0.01 0.08 0.05 0.09 0.06 0.07 0.07 0.09 0.04 0.07 0.07 0.07 0.07 0.05\n",
      "Time: 32.253074169158936\n",
      "Total Time: 2246.656444787979\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 74 / 200\n",
      "Score: -32.87001381069422\n",
      "Score: -677.3550275396556\n",
      "Training Team 1:\n",
      "Loss: 0.04 0.04 0.07 0.05 0.11 0.07 0.06 0.09 0.07 0.09 0.09 0.05 0.08 0.09 0.1 0.04 0.11 0.13 0.08 0.08\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.13 0.09 0.07 0.11 0.06 0.06 0.12 0.09 0.12 0.13 0.11 0.09 0.11 0.08 0.05 0.09 0.08 0.16 0.09\n",
      "Time: 32.1373496055603\n",
      "Total Time: 2278.7938544750214\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 75 / 200\n",
      "Score: -1056.0550306504592\n",
      "Score: -943.5650234408677\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.16 0.07 0.13 0.14 0.09 0.12 0.11 0.06 0.12 0.04 0.13 0.07 0.09 0.13 0.13 0.13 0.1 0.18 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.09 0.07 0.07 0.09 0.11 0.07 0.1 0.12 0.06 0.1 0.09 0.08 0.09 0.08 0.11 0.12 0.11 0.1 0.14\n",
      "Time: 32.09388494491577\n",
      "Total Time: 2310.8878016471863\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 76 / 200\n",
      "Score: -1013.9450234677643\n",
      "Score: -1058.3000321350992\n",
      "Training Team 1:\n",
      "Loss: 0.07 0.13 0.16 0.16 0.17 0.23 0.21 0.07 0.3 0.37 0.23 0.15 0.27 0.21 0.11 0.19 0.14 0.12 0.17 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.19 0.12 0.13 0.13 0.12 0.1 0.09 0.1 0.08 0.16 0.17 0.16 0.16 0.15 0.1 0.1 0.09 0.12 0.1\n",
      "Time: 32.41509938240051\n",
      "Total Time: 2343.30296087265\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 77 / 200\n",
      "Score: -621.360024029389\n",
      "Score: -402.0950188431889\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.1 0.18 0.12 0.12 0.17 0.2 0.09 0.14 0.16 0.23 0.12 0.17 0.16 0.14 0.23 0.24 0.12 0.16 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.17 0.1 0.15 0.15 0.16 0.08 0.14 0.11 0.21 0.18 0.14 0.15 0.13 0.12 0.14 0.12 0.11 0.16 0.16\n",
      "Time: 33.028242111206055\n",
      "Total Time: 2376.3312418460846\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 78 / 200\n",
      "Score: -355.5100164944306\n",
      "Score: -511.73002083133906\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.2 0.15 0.15 0.18 0.19 0.11 0.07 0.14 0.13 0.18 0.17 0.18 0.17 0.2 0.15 0.09 0.26 0.13 0.22\n",
      "Training Team 2:\n",
      "Loss: 0.17 0.15 0.17 0.19 0.12 0.12 0.13 0.16 0.17 0.08 0.14 0.16 0.15 0.09 0.16 0.13 0.16 0.11 0.2 0.25\n",
      "Time: 32.5679030418396\n",
      "Total Time: 2408.8991894721985\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 79 / 200\n",
      "Score: -1206.99002841115\n",
      "Score: -1176.6850247057155\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.08 0.15 0.17 0.11 0.16 0.1 0.13 0.1 0.16 0.13 0.15 0.17 0.16 0.14 0.13 0.12 0.12 0.17 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.16 0.16 0.19 0.14 0.12 0.09 0.13 0.17 0.12 0.18 0.13 0.14 0.15 0.16 0.14 0.15 0.19 0.15 0.13\n",
      "Time: 32.58855605125427\n",
      "Total Time: 2441.4877829551697\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 80 / 200\n",
      "Score: -351.83501872979105\n",
      "Score: -533.7050216719508\n",
      "Training Team 1:\n",
      "Loss: 0.23 0.14 0.1 0.14 0.17 0.11 0.15 0.17 0.18 0.15 0.19 0.2 0.2 0.13 0.15 0.14 0.18 0.11 0.11 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.18 0.25 0.19 0.16 0.25 0.18 0.24 0.16 0.19 0.27 0.14 0.13 0.18 0.17 0.19 0.17 0.15 0.16 0.23 0.17\n",
      "Time: 31.066283464431763\n",
      "Total Time: 2472.55411362648\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 81 / 200\n",
      "Score: -275.0050108395517\n",
      "Score: -1191.8000246603042\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.16 0.3 0.24 0.2 0.19 0.21 0.23 0.23 0.17 0.26 0.14 0.24 0.17 0.2 0.25 0.37 0.22 0.21 0.31\n",
      "Training Team 2:\n",
      "Loss: 0.18 0.19 0.13 0.16 0.16 0.14 0.26 0.28 0.2 0.27 0.22 0.15 0.23 0.46 0.44 0.25 0.54 0.87 0.56 0.17\n",
      "Time: 31.206042766571045\n",
      "Total Time: 2503.7602009773254\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 82 / 200\n",
      "Score: -1183.4000286534429\n",
      "Score: -1190.4100264962763\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.15 0.2 0.19 0.21 0.19 0.24 0.16 0.1 0.27 0.33 0.17 0.56 0.71 0.33 0.58 0.76 0.3 0.61 0.85\n",
      "Training Team 2:\n",
      "Loss: 0.19 0.18 0.15 0.28 0.27 0.35 0.54 0.44 0.28 0.2 0.28 0.28 0.18 0.28 0.35 0.26 0.17 0.27 0.34 0.26\n",
      "Time: 31.589019060134888\n",
      "Total Time: 2535.3492665290833\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 83 / 200\n",
      "Score: -1004.9800228960812\n",
      "Score: -1041.7200220646337\n",
      "Training Team 1:\n",
      "Loss: 0.22 0.23 0.3 0.27 0.16 0.19 0.31 0.35 0.23 0.21 0.64 0.74 0.25 0.81 0.93 0.42 0.52 0.67 0.22 0.82\n",
      "Training Team 2:\n",
      "Loss: 0.22 0.23 0.23 0.19 0.34 0.35 0.33 0.19 0.15 0.22 0.14 0.17 0.21 0.16 0.15 0.16 0.16 0.14 0.14 0.11\n",
      "Time: 33.14478397369385\n",
      "Total Time: 2568.494113445282\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 84 / 200\n",
      "Score: -645.3300152616575\n",
      "Score: -778.2850150121376\n",
      "Training Team 1:\n",
      "Loss: 1.06 1.07 1.07 1.07 1.13 1.14 0.77 0.13 0.97 1.4 0.71 0.44 0.81 0.23 0.78 1.14 0.81 0.32 0.56 0.95\n",
      "Training Team 2:\n",
      "Loss: 0.18 0.16 0.19 0.17 0.29 0.26 0.29 0.16 0.17 0.24 0.25 0.12 0.12 0.21 0.18 0.1 0.14 0.24 0.25 0.13\n",
      "Time: 33.32927393913269\n",
      "Total Time: 2601.8234322071075\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 85 / 200\n",
      "Score: -835.5450243195519\n",
      "Score: -586.0150160435587\n",
      "Training Team 1:\n",
      "Loss: 0.81 0.7 0.7 0.68 0.66 0.64 0.24 0.67 0.85 0.63 0.36 0.53 0.61 0.54 0.4 0.38 0.25 0.23 0.27 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.12 0.14 0.14 0.23 0.27 0.31 0.2 0.14 0.12 0.11 0.13 0.12 0.07 0.19 0.12 0.18 0.09 0.17 0.1\n",
      "Time: 33.09543776512146\n",
      "Total Time: 2634.9189336299896\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 86 / 200\n",
      "Score: -660.8750257510692\n",
      "Score: -933.9150174064562\n",
      "Training Team 1:\n",
      "Loss: 0.35 0.35 0.4 0.37 0.34 0.38 0.47 0.2 0.27 0.44 0.34 0.19 0.51 0.73 0.54 0.54 0.53 0.36 0.32 0.39\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.09 0.07 0.07 0.11 0.09 0.06 0.06 0.04 0.04 0.12 0.07 0.12 0.11 0.08 0.11 0.11 0.07 0.08 0.06\n",
      "Time: 33.3712739944458\n",
      "Total Time: 2668.2902743816376\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 87 / 200\n",
      "Score: -497.8650179570541\n",
      "Score: -789.735012980178\n",
      "Training Team 1:\n",
      "Loss: 0.37 0.37 0.31 0.32 0.45 0.47 0.43 0.24 0.28 0.23 0.24 0.28 0.22 0.26 0.27 0.19 0.29 0.17 0.25 0.17\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.08 0.05 0.05 0.03 0.05 0.06 0.08 0.08 0.05 0.05 0.02 0.04 0.05 0.04 0.03 0.08 0.07 0.06 0.05\n",
      "Time: 33.00306010246277\n",
      "Total Time: 2701.2933790683746\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 88 / 200\n",
      "Score: -641.3100161058828\n",
      "Score: -1076.670020647347\n",
      "Training Team 1:\n",
      "Loss: 0.22 0.26 0.21 0.19 0.22 0.18 0.19 0.16 0.14 0.25 0.16 0.13 0.11 0.19 0.23 0.25 0.18 0.21 0.18 0.23\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.05 0.05 0.03 0.07 0.06 0.1 0.06 0.06 0.04 0.04 0.07 0.02 0.09 0.06 0.04 0.04 0.06 0.06 0.07\n",
      "Time: 32.710678815841675\n",
      "Total Time: 2734.004102706909\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 89 / 200\n",
      "Score: -745.3500142227858\n",
      "Score: -1049.3000227855518\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.19 0.15 0.2 0.16 0.18 0.19 0.15 0.16 0.18 0.2 0.14 0.12 0.21 0.14 0.14 0.21 0.13 0.12 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.02 0.06 0.03 0.02 0.03 0.02 0.02 0.01 0.01 0.04 0.06 0.03 0.04 0.05 0.08 0.03 0.03 0.01 0.04\n",
      "Time: 32.588756799697876\n",
      "Total Time: 2766.5929012298584\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 90 / 200\n",
      "Score: -705.9700191011652\n",
      "Score: -856.9550162171945\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.13 0.15 0.16 0.15 0.19 0.19 0.14 0.12 0.11 0.16 0.17 0.14 0.16 0.14 0.15 0.2 0.28 0.16 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.01 0.02 0.04 0.04 0.05 0.07 0.01 0.05 0.03 0.04 0.03 0.01 0.06 0.03 0.01 0.02 0.05 0.07 0.02 0.03\n",
      "Time: 33.03342390060425\n",
      "Total Time: 2799.626373529434\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 91 / 200\n",
      "Score: -451.9750192211941\n",
      "Score: -371.4550101365894\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.16 0.21 0.12 0.13 0.18 0.12 0.19 0.12 0.09 0.19 0.26 0.2 0.15 0.23 0.19 0.16 0.19 0.2 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.03 0.06 0.13 0.04 0.08 0.05 0.05 0.05 0.06 0.06 0.05 0.05 0.01 0.03 0.02 0.04 0.02 0.08 0.08\n",
      "Time: 32.76135802268982\n",
      "Total Time: 2832.387773990631\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 92 / 200\n",
      "Score: -525.9750189492479\n",
      "Score: -760.6000166982412\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.17 0.22 0.14 0.29 0.18 0.21 0.2 0.21 0.16 0.18 0.25 0.23 0.2 0.18 0.26 0.18 0.2 0.12 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.11 0.09 0.12 0.07 0.08 0.08 0.03 0.07 0.06 0.06 0.09 0.1 0.05 0.07 0.05 0.1 0.06 0.07 0.04\n",
      "Time: 33.34663367271423\n",
      "Total Time: 2865.7344517707825\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 93 / 200\n",
      "Score: -420.7600148599595\n",
      "Score: -112.12500806991011\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.19 0.2 0.3 0.22 0.24 0.22 0.24 0.23 0.26 0.26 0.19 0.2 0.23 0.34 0.24 0.16 0.24 0.16 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.08 0.07 0.08 0.07 0.09 0.08 0.1 0.09 0.06 0.12 0.05 0.09 0.08 0.1 0.16 0.06 0.08 0.1 0.07\n",
      "Time: 32.83782172203064\n",
      "Total Time: 2898.5723147392273\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 94 / 200\n",
      "Score: -248.51001302339137\n",
      "Score: -637.3150098426268\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.25 0.21 0.25 0.18 0.2 0.26 0.24 0.2 0.26 0.24 0.18 0.25 0.26 0.25 0.31 0.29 0.25 0.22 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.03 0.14 0.09 0.04 0.06 0.07 0.1 0.04 0.06 0.1 0.1 0.13 0.07 0.06 0.11 0.05 0.08 0.07 0.07\n",
      "Time: 33.9399049282074\n",
      "Total Time: 2932.5122594833374\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 95 / 200\n",
      "Score: 133.94999130163342\n",
      "Score: 46.89998936466873\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.2 0.26 0.33 0.31 0.21 0.29 0.19 0.19 0.2 0.27 0.29 0.27 0.2 0.2 0.3 0.33 0.27 0.32 0.33\n",
      "Training Team 2:\n",
      "Loss: 0.13 0.12 0.16 0.12 0.13 0.14 0.08 0.17 0.17 0.16 0.1 0.1 0.18 0.12 0.11 0.08 0.17 0.13 0.14 0.08\n",
      "Time: 29.702277898788452\n",
      "Total Time: 2962.2145931720734\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 96 / 200\n",
      "Score: -439.86001061461866\n",
      "Score: -844.8000097889453\n",
      "Training Team 1:\n",
      "Loss: 0.32 0.26 0.23 0.23 0.24 0.24 0.3 0.22 0.18 0.25 0.26 0.16 0.3 0.32 0.24 0.18 0.19 0.28 0.37 0.3\n",
      "Training Team 2:\n",
      "Loss: 0.07 0.14 0.13 0.08 0.06 0.08 0.1 0.08 0.07 0.1 0.1 0.09 0.1 0.11 0.07 0.11 0.14 0.18 0.1 0.08\n",
      "Time: 33.47919178009033\n",
      "Total Time: 2995.6938302516937\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 97 / 200\n",
      "Score: 2.8099919389933348\n",
      "Score: -46.390007160604\n",
      "Training Team 1:\n",
      "Loss: 0.35 0.25 0.27 0.27 0.23 0.45 0.35 0.26 0.25 0.19 0.28 0.27 0.39 0.3 0.21 0.27 0.25 0.3 0.35 0.3\n",
      "Training Team 2:\n",
      "Loss: 0.13 0.19 0.23 0.11 0.13 0.12 0.23 0.08 0.05 0.12 0.16 0.09 0.13 0.25 0.11 0.16 0.15 0.2 0.12 0.14\n",
      "Time: 32.76405334472656\n",
      "Total Time: 3028.457941055298\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 98 / 200\n",
      "Score: -567.8850095132366\n",
      "Score: -796.1000077025965\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.33 0.24 0.24 0.27 0.28 0.21 0.3 0.25 0.31 0.28 0.14 0.28 0.26 0.27 0.32 0.25 0.27 0.31 0.26\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.12 0.13 0.1 0.13 0.15 0.09 0.13 0.16 0.18 0.16 0.13 0.17 0.19 0.14 0.13 0.15 0.11 0.14 0.11\n",
      "Time: 33.30091333389282\n",
      "Total Time: 3061.758892059326\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 99 / 200\n",
      "Score: 25.404992839321494\n",
      "Score: -14.910007850266993\n",
      "Training Team 1:\n",
      "Loss: 0.41 0.23 0.37 0.26 0.33 0.29 0.27 0.29 0.27 0.29 0.26 0.3 0.25 0.36 0.34 0.29 0.3 0.31 0.35 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.15 0.14 0.15 0.18 0.08 0.16 0.14 0.13 0.14 0.23 0.11 0.16 0.1 0.19 0.13 0.19 0.16 0.31 0.16\n",
      "Time: 31.519869565963745\n",
      "Total Time: 3093.2788302898407\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 100 / 200\n",
      "Score: -699.9400050779805\n",
      "Score: -789.5000079562888\n",
      "Training Team 1:\n",
      "Loss: 0.29 0.3 0.26 0.44 0.39 0.45 0.42 0.35 0.43 0.35 0.32 0.28 0.27 0.21 0.3 0.35 0.34 0.33 0.31 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.17 0.1 0.11 0.12 0.08 0.15 0.13 0.14 0.13 0.06 0.1 0.11 0.18 0.2 0.1 0.17 0.16 0.17 0.08\n",
      "Time: 33.70460510253906\n",
      "Total Time: 3126.983480453491\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 101 / 200\n",
      "Score: -711.5000040782616\n",
      "Score: -384.60000591445714\n",
      "Training Team 1:\n",
      "Loss: 0.29 0.31 0.26 0.25 0.29 0.27 0.25 0.21 0.25 0.25 0.21 0.28 0.29 0.22 0.23 0.25 0.26 0.25 0.22 0.23\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.14 0.11 0.22 0.15 0.23 0.25 0.11 0.13 0.2 0.1 0.16 0.14 0.15 0.17 0.12 0.14 0.12 0.15 0.15\n",
      "Time: 34.913573265075684\n",
      "Total Time: 3161.897105693817\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 102 / 200\n",
      "Score: -77.58500811085105\n",
      "Score: -321.49500733986497\n",
      "Training Team 1:\n",
      "Loss: 0.39 0.37 0.19 0.38 0.24 0.23 0.24 0.23 0.3 0.29 0.2 0.24 0.24 0.24 0.28 0.21 0.26 0.24 0.3 0.41\n",
      "Training Team 2:\n",
      "Loss: 0.2 0.16 0.23 0.21 0.19 0.2 0.2 0.21 0.18 0.23 0.29 0.31 0.19 0.24 0.42 0.28 0.71 0.38 0.33 0.14\n",
      "Time: 37.07528734207153\n",
      "Total Time: 3198.9724411964417\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 103 / 200\n",
      "Score: -539.490008209832\n",
      "Score: -753.4000058732927\n",
      "Training Team 1:\n",
      "Loss: 0.45 0.41 0.53 0.32 0.58 0.58 0.47 0.47 0.48 0.27 0.3 0.6 0.45 0.53 0.33 0.3 0.37 0.41 0.36 0.39\n",
      "Training Team 2:\n",
      "Loss: 0.26 0.21 0.26 0.18 0.15 0.27 0.25 0.35 0.36 0.25 0.54 0.55 0.44 0.34 0.29 0.19 0.15 0.37 0.4 0.27\n",
      "Time: 36.63929486274719\n",
      "Total Time: 3235.611787557602\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 104 / 200\n",
      "Score: -245.43001866992563\n",
      "Score: 27.989991110749543\n",
      "Training Team 1:\n",
      "Loss: 0.35 0.33 0.44 0.36 0.4 0.33 0.39 0.4 0.22 0.2 0.2 0.27 0.25 0.24 0.21 0.27 0.19 0.22 0.24 0.32\n",
      "Training Team 2:\n",
      "Loss: 0.35 0.39 0.29 0.34 0.27 0.32 0.31 0.22 0.28 0.31 0.23 0.22 0.28 0.21 0.21 0.18 0.24 0.25 0.21 0.18\n",
      "Time: 31.584895849227905\n",
      "Total Time: 3267.196729183197\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 105 / 200\n",
      "Score: -301.0300177000463\n",
      "Score: -678.600002668798\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.24 0.26 0.32 0.28 0.32 0.25 0.37 0.3 0.27 0.27 0.28 0.29 0.2 0.34 0.24 0.23 0.27 0.19 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.27 0.14 0.16 0.21 0.23 0.3 0.24 0.24 0.29 0.18 0.24 0.19 0.2 0.18 0.15 0.23 0.1 0.19 0.17 0.23\n",
      "Time: 32.14259457588196\n",
      "Total Time: 3299.339368581772\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 106 / 200\n",
      "Score: -653.420011986047\n",
      "Score: -11.630008327774704\n",
      "Training Team 1:\n",
      "Loss: 0.31 0.37 0.21 0.22 0.24 0.25 0.24 0.13 0.3 0.22 0.19 0.24 0.26 0.25 0.19 0.2 0.2 0.19 0.23 0.22\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.18 0.15 0.14 0.17 0.2 0.16 0.1 0.18 0.2 0.18 0.2 0.2 0.19 0.15 0.13 0.16 0.19 0.14 0.19\n",
      "Time: 35.16769003868103\n",
      "Total Time: 3334.507098197937\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 107 / 200\n",
      "Score: -519.0400174474344\n",
      "Score: -296.2550084097311\n",
      "Training Team 1:\n",
      "Loss: 0.28 0.27 0.19 0.32 0.18 0.23 0.24 0.21 0.19 0.16 0.19 0.21 0.23 0.16 0.17 0.24 0.16 0.2 0.27 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.21 0.21 0.19 0.24 0.18 0.15 0.22 0.11 0.24 0.29 0.19 0.25 0.16 0.23 0.19 0.25 0.2 0.16 0.14 0.16\n",
      "Time: 35.285282373428345\n",
      "Total Time: 3369.7924258708954\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 108 / 200\n",
      "Score: -588.3400089247152\n",
      "Score: -726.40000471659\n",
      "Training Team 1:\n",
      "Loss: 0.22 0.18 0.19 0.24 0.27 0.22 0.2 0.19 0.25 0.22 0.21 0.19 0.13 0.16 0.2 0.16 0.2 0.2 0.24 0.23\n",
      "Training Team 2:\n",
      "Loss: 0.19 0.15 0.14 0.12 0.2 0.11 0.18 0.18 0.22 0.18 0.17 0.17 0.18 0.15 0.12 0.22 0.09 0.16 0.18 0.14\n",
      "Time: 34.7399001121521\n",
      "Total Time: 3404.532364845276\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 109 / 200\n",
      "Score: -223.04001289978623\n",
      "Score: -378.30500874016434\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.22 0.17 0.22 0.22 0.19 0.2 0.23 0.16 0.18 0.16 0.18 0.18 0.23 0.16 0.17 0.21 0.15 0.2 0.17\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.27 0.15 0.21 0.22 0.16 0.19 0.17 0.19 0.2 0.13 0.21 0.15 0.23 0.21 0.18 0.19 0.17 0.2 0.16\n",
      "Time: 34.74836587905884\n",
      "Total Time: 3439.280773162842\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 110 / 200\n",
      "Score: -32.37000628281385\n",
      "Score: -652.8000015635043\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.25 0.21 0.19 0.35 0.19 0.19 0.21 0.19 0.2 0.22 0.23 0.17 0.17 0.14 0.21 0.26 0.27 0.21 0.24\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.18 0.15 0.11 0.15 0.16 0.21 0.19 0.16 0.24 0.1 0.1 0.14 0.11 0.14 0.22 0.2 0.24 0.11 0.23\n",
      "Time: 34.79899787902832\n",
      "Total Time: 3474.0798082351685\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 111 / 200\n",
      "Score: -88.07500848267227\n",
      "Score: -489.540007263422\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.25 0.23 0.26 0.25 0.21 0.24 0.21 0.22 0.2 0.22 0.33 0.27 0.23 0.22 0.14 0.21 0.21 0.25 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.21 0.16 0.17 0.08 0.22 0.16 0.15 0.12 0.24 0.17 0.14 0.17 0.16 0.17 0.16 0.18 0.21 0.11 0.21 0.2\n",
      "Time: 34.27491283416748\n",
      "Total Time: 3508.354766368866\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 112 / 200\n",
      "Score: 46.17499223072082\n",
      "Score: -553.3000044925138\n",
      "Training Team 1:\n",
      "Loss: 0.27 0.26 0.26 0.27 0.29 0.2 0.26 0.25 0.26 0.18 0.26 0.22 0.2 0.21 0.28 0.24 0.23 0.31 0.24 0.26\n",
      "Training Team 2:\n",
      "Loss: 0.18 0.07 0.15 0.18 0.1 0.18 0.13 0.22 0.13 0.25 0.15 0.14 0.16 0.2 0.14 0.16 0.18 0.23 0.13 0.11\n",
      "Time: 34.37920141220093\n",
      "Total Time: 3542.7340099811554\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 113 / 200\n",
      "Score: 23.274989679455757\n",
      "Score: -320.52000809367746\n",
      "Training Team 1:\n",
      "Loss: 0.31 0.22 0.33 0.25 0.33 0.21 0.2 0.28 0.37 0.34 0.28 0.27 0.4 0.29 0.27 0.25 0.27 0.22 0.26 0.24\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.19 0.22 0.15 0.18 0.23 0.15 0.21 0.21 0.14 0.14 0.18 0.17 0.2 0.18 0.2 0.14 0.13 0.12 0.22\n",
      "Time: 34.81787085533142\n",
      "Total Time: 3577.5519709587097\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 114 / 200\n",
      "Score: -455.1700120167807\n",
      "Score: -189.35500665288419\n",
      "Training Team 1:\n",
      "Loss: 0.35 0.28 0.29 0.21 0.28 0.33 0.34 0.26 0.27 0.27 0.2 0.3 0.36 0.26 0.32 0.28 0.21 0.32 0.3 0.23\n",
      "Training Team 2:\n",
      "Loss: 0.2 0.2 0.29 0.15 0.22 0.22 0.19 0.22 0.2 0.24 0.18 0.18 0.26 0.11 0.18 0.26 0.17 0.26 0.17 0.26\n",
      "Time: 34.43186330795288\n",
      "Total Time: 3611.9838995933533\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 115 / 200\n",
      "Score: -348.1000043749809\n",
      "Score: -190.31000961735845\n",
      "Training Team 1:\n",
      "Loss: 0.3 0.31 0.27 0.29 0.28 0.34 0.38 0.21 0.25 0.24 0.17 0.35 0.41 0.31 0.26 0.22 0.17 0.33 0.47 0.39\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.2 0.16 0.13 0.24 0.18 0.15 0.22 0.15 0.23 0.19 0.19 0.15 0.16 0.22 0.22 0.29 0.25 0.17 0.2\n",
      "Time: 34.09941554069519\n",
      "Total Time: 3646.0833570957184\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 116 / 200\n",
      "Score: -52.01000955328345\n",
      "Score: -680.5000027501956\n",
      "Training Team 1:\n",
      "Loss: 0.28 0.26 0.3 0.28 0.44 0.46 0.46 0.43 0.25 0.31 0.28 0.29 0.21 0.19 0.27 0.16 0.14 0.21 0.19 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.2 0.14 0.18 0.13 0.16 0.13 0.16 0.19 0.16 0.12 0.14 0.16 0.19 0.15 0.15 0.08 0.16 0.11 0.2\n",
      "Time: 33.95181584358215\n",
      "Total Time: 3680.035215854645\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 117 / 200\n",
      "Score: -580.780006497167\n",
      "Score: -708.3000039411709\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.28 0.18 0.28 0.23 0.31 0.23 0.32 0.19 0.22 0.2 0.23 0.24 0.22 0.19 0.23 0.26 0.2 0.22 0.26\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.11 0.13 0.13 0.16 0.18 0.16 0.15 0.16 0.14 0.14 0.19 0.12 0.17 0.16 0.14 0.14 0.17 0.17 0.12\n",
      "Time: 34.5630989074707\n",
      "Total Time: 3714.5983674526215\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 118 / 200\n",
      "Score: -204.29000790882856\n",
      "Score: -377.39500672277063\n",
      "Training Team 1:\n",
      "Loss: 0.21 0.26 0.22 0.26 0.25 0.23 0.24 0.24 0.25 0.2 0.19 0.21 0.19 0.2 0.19 0.21 0.22 0.2 0.17 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.14 0.13 0.14 0.19 0.15 0.21 0.18 0.16 0.16 0.13 0.14 0.16 0.13 0.13 0.09 0.13 0.12 0.1 0.1\n",
      "Time: 33.98262166976929\n",
      "Total Time: 3748.5810334682465\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 119 / 200\n",
      "Score: -281.9050185009837\n",
      "Score: 59.84999356046319\n",
      "Training Team 1:\n",
      "Loss: 0.22 0.24 0.24 0.18 0.22 0.19 0.19 0.22 0.14 0.2 0.19 0.21 0.3 0.2 0.22 0.2 0.21 0.2 0.29 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.21 0.2 0.15 0.16 0.16 0.18 0.24 0.18 0.19 0.19 0.28 0.2 0.16 0.13 0.14 0.18 0.24 0.24 0.21\n",
      "Time: 33.820446252822876\n",
      "Total Time: 3782.401524782181\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 120 / 200\n",
      "Score: -263.32500618044287\n",
      "Score: -773.0000067129731\n",
      "Training Team 1:\n",
      "Loss: 0.35 0.25 0.3 0.23 0.31 0.25 0.35 0.33 0.21 0.37 0.53 0.52 0.28 0.28 0.48 0.21 0.33 0.36 0.19 0.4\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.2 0.23 0.19 0.11 0.15 0.17 0.18 0.18 0.16 0.11 0.1 0.17 0.17 0.07 0.14 0.25 0.17 0.18 0.19\n",
      "Time: 33.57357406616211\n",
      "Total Time: 3815.975153207779\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 121 / 200\n",
      "Score: -24.650006654672325\n",
      "Score: -403.3000098289922\n",
      "Training Team 1:\n",
      "Loss: 0.49 0.52 0.54 0.56 0.47 0.53 0.59 0.33 0.18 0.24 0.2 0.12 0.4 0.85 0.47 0.36 0.31 0.26 0.31 0.34\n",
      "Training Team 2:\n",
      "Loss: 0.25 0.28 0.23 0.18 0.19 0.18 0.22 0.18 0.13 0.21 0.14 0.19 0.18 0.16 0.18 0.16 0.17 0.17 0.17 0.23\n",
      "Time: 34.42460823059082\n",
      "Total Time: 3850.3998098373413\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 122 / 200\n",
      "Score: -148.24000216368586\n",
      "Score: -696.3000034270808\n",
      "Training Team 1:\n",
      "Loss: 0.87 0.94 0.91 0.92 0.8 0.74 0.7 1.08 1.19 0.58 1.15 1.44 0.96 0.72 0.91 0.48 0.82 1.01 0.42 0.48\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.22 0.19 0.26 0.19 0.21 0.18 0.13 0.13 0.15 0.14 0.18 0.18 0.15 0.09 0.16 0.11 0.13 0.15 0.15\n",
      "Time: 34.06238865852356\n",
      "Total Time: 3884.4622523784637\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 123 / 200\n",
      "Score: -734.0000050421804\n",
      "Score: 85.96499140001833\n",
      "Training Team 1:\n",
      "Loss: 0.74 0.79 0.76 0.78 0.84 0.77 0.74 0.25 0.78 0.85 0.74 0.26 0.71 0.69 0.19 0.37 0.46 0.21 0.88 1.57\n",
      "Training Team 2:\n",
      "Loss: 0.17 0.14 0.17 0.16 0.15 0.15 0.14 0.14 0.16 0.12 0.19 0.15 0.13 0.18 0.12 0.2 0.15 0.16 0.08 0.15\n",
      "Time: 33.95635414123535\n",
      "Total Time: 3918.418651819229\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 124 / 200\n",
      "Score: -542.5000253431499\n",
      "Score: -472.2250091684982\n",
      "Training Team 1:\n",
      "Loss: 1.33 1.31 1.36 1.35 1.31 1.33 1.32 0.39 1.1 1.82 1.59 0.82 0.22 0.43 0.24 0.52 0.75 0.42 0.51 0.69\n",
      "Training Team 2:\n",
      "Loss: 0.19 0.13 0.15 0.16 0.15 0.13 0.16 0.1 0.11 0.15 0.14 0.12 0.15 0.15 0.11 0.1 0.15 0.11 0.16 0.11\n",
      "Time: 34.19705629348755\n",
      "Total Time: 3952.6157443523407\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 125 / 200\n",
      "Score: -374.43502325285226\n",
      "Score: -652.5800031814724\n",
      "Training Team 1:\n",
      "Loss: 0.41 0.42 0.37 0.41 0.4 0.41 0.29 0.3 0.27 0.28 0.22 0.16 0.21 0.23 0.26 0.25 0.24 0.42 0.6 0.55\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.09 0.17 0.13 0.14 0.14 0.12 0.15 0.12 0.16 0.14 0.07 0.09 0.14 0.11 0.09 0.09 0.08 0.11 0.09\n",
      "Time: 34.407721281051636\n",
      "Total Time: 3987.0235056877136\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 126 / 200\n",
      "Score: -308.30000562313944\n",
      "Score: -486.50000626873225\n",
      "Training Team 1:\n",
      "Loss: 0.36 0.4 0.39 0.42 0.4 0.46 0.53 0.52 0.45 0.35 0.21 0.22 0.2 0.17 0.21 0.19 0.14 0.08 0.18 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.15 0.13 0.13 0.12 0.15 0.13 0.1 0.15 0.16 0.11 0.18 0.14 0.15 0.12 0.14 0.11 0.11 0.12 0.12\n",
      "Time: 34.02786898612976\n",
      "Total Time: 4021.0514204502106\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 127 / 200\n",
      "Score: -357.98500773124397\n",
      "Score: -487.2800036231056\n",
      "Training Team 1:\n",
      "Loss: 0.21 0.19 0.22 0.19 0.23 0.22 0.18 0.17 0.14 0.2 0.14 0.12 0.21 0.19 0.15 0.19 0.13 0.17 0.17 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.1 0.08 0.14 0.13 0.12 0.09 0.09 0.08 0.12 0.09 0.09 0.1 0.11 0.1 0.11 0.12 0.1 0.11 0.1\n",
      "Time: 34.15863084793091\n",
      "Total Time: 4055.210096359253\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 128 / 200\n",
      "Score: -205.84000370372087\n",
      "Score: -678.3000026559457\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.14 0.18 0.18 0.2 0.17 0.17 0.16 0.13 0.16 0.24 0.2 0.2 0.22 0.25 0.15 0.08 0.15 0.12 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.09 0.1 0.09 0.09 0.1 0.06 0.12 0.07 0.08 0.07 0.08 0.09 0.1 0.07 0.07 0.06 0.07 0.08 0.06\n",
      "Time: 34.012664794921875\n",
      "Total Time: 4089.2227926254272\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 129 / 200\n",
      "Score: -683.9800058286637\n",
      "Score: -647.5000094836578\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.14 0.14 0.11 0.19 0.17 0.13 0.15 0.17 0.17 0.13 0.13 0.13 0.1 0.1 0.12 0.09 0.11 0.12 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.09 0.06 0.1 0.07 0.11 0.11 0.08 0.08 0.09 0.08 0.08 0.07 0.08 0.07 0.08 0.07 0.09 0.08 0.06\n",
      "Time: 34.65022301673889\n",
      "Total Time: 4123.873056173325\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 130 / 200\n",
      "Score: -89.25000635441393\n",
      "Score: -337.26500794570893\n",
      "Training Team 1:\n",
      "Loss: 0.09 0.08 0.08 0.09 0.11 0.13 0.11 0.13 0.13 0.11 0.11 0.1 0.09 0.14 0.11 0.09 0.1 0.08 0.07 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.06 0.07 0.09 0.08 0.09 0.08 0.1 0.08 0.06 0.1 0.09 0.06 0.07 0.07 0.11 0.11 0.07 0.06 0.06\n",
      "Time: 34.257179260253906\n",
      "Total Time: 4158.130275726318\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 131 / 200\n",
      "Score: -723.9650082550943\n",
      "Score: -681.8000028058887\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.12 0.1 0.12 0.11 0.1 0.09 0.1 0.07 0.14 0.09 0.1 0.08 0.12 0.09 0.08 0.06 0.08 0.09 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.08 0.09 0.06 0.08 0.08 0.08 0.06 0.08 0.08 0.06 0.08 0.07 0.07 0.07 0.09 0.07 0.06 0.05 0.07\n",
      "Time: 34.659167528152466\n",
      "Total Time: 4192.789477586746\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 132 / 200\n",
      "Score: -427.9200060358271\n",
      "Score: -690.4000063706189\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.09 0.09 0.11 0.14 0.09 0.12 0.13 0.1 0.07 0.07 0.1 0.1 0.11 0.08 0.1 0.1 0.11 0.08 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.07 0.04 0.06 0.06 0.04 0.08 0.05 0.07 0.04 0.03 0.08 0.05 0.07 0.02 0.05 0.03 0.05 0.07 0.04\n",
      "Time: 34.64900588989258\n",
      "Total Time: 4227.43852686882\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 133 / 200\n",
      "Score: -464.7250034864992\n",
      "Score: -601.0000003892928\n",
      "Training Team 1:\n",
      "Loss: 0.06 0.1 0.14 0.11 0.07 0.09 0.07 0.12 0.13 0.11 0.05 0.07 0.12 0.07 0.06 0.1 0.06 0.06 0.08 0.07\n",
      "Training Team 2:\n",
      "Loss: 0.05 0.05 0.04 0.04 0.06 0.06 0.06 0.05 0.06 0.04 0.03 0.04 0.03 0.06 0.04 0.05 0.03 0.04 0.06 0.05\n",
      "Time: 35.57644963264465\n",
      "Total Time: 4263.01501750946\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 134 / 200\n",
      "Score: -627.5400092471391\n",
      "Score: -373.74500893987715\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.08 0.11 0.09 0.08 0.09 0.1 0.07 0.12 0.07 0.1 0.08 0.07 0.08 0.06 0.07 0.1 0.11 0.07 0.07\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.07 0.05 0.06 0.06 0.05 0.05 0.04 0.07 0.07 0.04 0.06 0.05 0.05 0.05 0.04 0.04 0.07 0.07 0.06\n",
      "Time: 35.80419206619263\n",
      "Total Time: 4298.819264411926\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 135 / 200\n",
      "Score: -49.41000625025481\n",
      "Score: -301.1150082917884\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.11 0.11 0.14 0.11 0.13 0.09 0.14 0.11 0.12 0.09 0.08 0.08 0.12 0.09 0.06 0.07 0.1 0.09 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.05 0.08 0.07 0.06 0.05 0.05 0.09 0.07 0.06 0.08 0.13 0.06 0.07 0.05 0.05 0.15 0.2 0.18 0.19\n",
      "Time: 35.37857460975647\n",
      "Total Time: 4334.19788980484\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 136 / 200\n",
      "Score: -430.2150118276477\n",
      "Score: -352.4650047728792\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.11 0.12 0.1 0.09 0.15 0.12 0.1 0.08 0.13 0.11 0.16 0.13 0.14 0.12 0.09 0.12 0.11 0.14 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.24 0.27 0.23 0.3 0.23 0.25 0.18 0.13 0.14 0.12 0.1 0.12 0.13 0.18 0.1 0.13 0.22 0.16 0.1 0.13\n",
      "Time: 35.232404708862305\n",
      "Total Time: 4369.430328845978\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 137 / 200\n",
      "Score: -724.7000046437606\n",
      "Score: -434.03500308282673\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.09 0.08 0.15 0.13 0.09 0.13 0.11 0.09 0.12 0.09 0.08 0.09 0.12 0.11 0.16 0.11 0.11 0.1 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.12 0.13 0.11 0.1 0.09 0.11 0.08 0.07 0.11 0.07 0.11 0.08 0.07 0.1 0.06 0.07 0.08 0.12 0.07\n",
      "Time: 35.51600694656372\n",
      "Total Time: 4404.946374416351\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 138 / 200\n",
      "Score: -231.50000760145485\n",
      "Score: -664.8000034298748\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.14 0.12 0.11 0.13 0.16 0.09 0.14 0.21 0.11 0.1 0.14 0.1 0.12 0.13 0.12 0.1 0.16 0.13 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.08 0.09 0.07 0.06 0.07 0.07 0.07 0.06 0.07 0.09 0.1 0.08 0.1 0.09 0.08 0.05 0.07 0.05 0.06\n",
      "Time: 34.855316400527954\n",
      "Total Time: 4439.801722288132\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 139 / 200\n",
      "Score: -638.9900024179369\n",
      "Score: -389.7000134848058\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.08 0.1 0.1 0.09 0.11 0.11 0.09 0.09 0.07 0.1 0.12 0.07 0.08 0.1 0.11 0.09 0.06 0.09 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.08 0.07 0.09 0.07 0.08 0.05 0.1 0.09 0.08 0.06 0.07 0.06 0.09 0.08 0.08 0.05 0.08 0.09 0.08\n",
      "Time: 35.266878843307495\n",
      "Total Time: 4475.068643331528\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 140 / 200\n",
      "Score: -98.75500548165292\n",
      "Score: -263.53000815492123\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.13 0.11 0.07 0.09 0.15 0.08 0.1 0.09 0.13 0.09 0.13 0.11 0.12 0.1 0.1 0.13 0.12 0.12 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.07 0.07 0.07 0.09 0.08 0.08 0.06 0.09 0.09 0.11 0.08 0.08 0.07 0.1 0.07 0.08 0.08 0.07 0.1\n",
      "Time: 34.09373617172241\n",
      "Total Time: 4509.162422180176\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 141 / 200\n",
      "Score: -663.6000020261854\n",
      "Score: -219.58001092728227\n",
      "Training Team 1:\n",
      "Loss: 0.12 0.11 0.08 0.1 0.14 0.1 0.09 0.13 0.08 0.1 0.1 0.09 0.12 0.12 0.1 0.09 0.09 0.11 0.11 0.08\n",
      "Training Team 2:\n",
      "Loss: 0.08 0.09 0.08 0.14 0.06 0.08 0.08 0.06 0.07 0.05 0.07 0.08 0.09 0.07 0.09 0.06 0.07 0.07 0.07 0.07\n",
      "Time: 33.012152671813965\n",
      "Total Time: 4542.174619436264\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 142 / 200\n",
      "Score: -12.995006357319653\n",
      "Score: -459.5050102407113\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.16 0.15 0.17 0.12 0.14 0.15 0.19 0.12 0.15 0.15 0.2 0.14 0.09 0.1 0.08 0.08 0.13 0.13 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.32 0.22 0.28 0.18 0.22 0.13 0.15 0.13 0.15 0.14 0.16 0.18 0.11 0.1 0.15 0.18 0.16 0.12 0.11\n",
      "Time: 32.77700638771057\n",
      "Total Time: 4574.951662540436\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 143 / 200\n",
      "Score: -538.990013805218\n",
      "Score: -142.68500902317464\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.14 0.14 0.12 0.13 0.14 0.12 0.09 0.06 0.15 0.1 0.12 0.1 0.12 0.08 0.09 0.15 0.11 0.13 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.1 0.12 0.14 0.15 0.1 0.11 0.11 0.13 0.1 0.11 0.13 0.08 0.11 0.09 0.11 0.11 0.17 0.19 0.17\n",
      "Time: 32.660813093185425\n",
      "Total Time: 4607.612519025803\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 144 / 200\n",
      "Score: -600.2400053292513\n",
      "Score: -424.36000926978886\n",
      "Training Team 1:\n",
      "Loss: 0.06 0.12 0.11 0.13 0.14 0.13 0.11 0.08 0.15 0.06 0.08 0.12 0.07 0.1 0.12 0.15 0.12 0.08 0.13 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.2 0.17 0.14 0.21 0.22 0.19 0.13 0.15 0.11 0.08 0.14 0.12 0.15 0.11 0.14 0.09 0.09 0.1 0.11 0.08\n",
      "Time: 32.61423873901367\n",
      "Total Time: 4640.226802825928\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 145 / 200\n",
      "Score: -228.19000628124923\n",
      "Score: -218.70001187268645\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.1 0.11 0.09 0.08 0.1 0.11 0.11 0.13 0.08 0.13 0.08 0.07 0.09 0.1 0.09 0.08 0.14 0.13 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.14 0.1 0.11 0.11 0.1 0.12 0.1 0.07 0.11 0.11 0.11 0.1 0.06 0.06 0.08 0.07 0.09 0.11 0.08\n",
      "Time: 32.21483254432678\n",
      "Total Time: 4672.441679954529\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 146 / 200\n",
      "Score: -466.48001387529075\n",
      "Score: -312.44501021970063\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.1 0.13 0.11 0.11 0.1 0.14 0.13 0.08 0.14 0.15 0.12 0.08 0.11 0.11 0.13 0.12 0.1 0.13 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.09 0.11 0.07 0.11 0.12 0.11 0.11 0.11 0.08 0.09 0.07 0.08 0.11 0.1 0.1 0.12 0.09 0.07 0.11\n",
      "Time: 32.99116277694702\n",
      "Total Time: 4705.432879686356\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 147 / 200\n",
      "Score: -219.3750063823536\n",
      "Score: -450.42500849533826\n",
      "Training Team 1:\n",
      "Loss: 0.11 0.12 0.12 0.15 0.15 0.14 0.11 0.12 0.1 0.11 0.12 0.11 0.08 0.14 0.17 0.13 0.13 0.13 0.13 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.13 0.16 0.14 0.12 0.1 0.13 0.11 0.11 0.14 0.19 0.2 0.2 0.2 0.23 0.28 0.23 0.1 0.28 0.47\n",
      "Time: 33.99111986160278\n",
      "Total Time: 4739.424061059952\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 148 / 200\n",
      "Score: -720.8000050969422\n",
      "Score: -692.1850045975298\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.22 0.21 0.18 0.19 0.11 0.28 0.38 0.38 0.13 0.67 0.97 0.53 0.11 0.19 0.11 0.64 1.06 0.83 0.58\n",
      "Training Team 2:\n",
      "Loss: 0.37 0.37 0.41 0.43 0.38 0.41 0.35 0.18 0.2 0.36 0.42 0.36 0.24 0.19 0.23 0.17 0.19 0.23 0.15 0.16\n",
      "Time: 33.23816514015198\n",
      "Total Time: 4772.6622676849365\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 149 / 200\n",
      "Score: -679.7850142875686\n",
      "Score: -128.49000653997064\n",
      "Training Team 1:\n",
      "Loss: 0.51 0.52 0.51 0.52 0.5 0.59 0.51 0.38 0.24 0.27 0.21 0.24 0.16 0.43 0.73 0.45 0.32 0.55 0.5 0.24\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.17 0.18 0.2 0.22 0.19 0.13 0.11 0.13 0.12 0.13 0.15 0.15 0.11 0.17 0.13 0.14 0.13 0.14 0.11\n",
      "Time: 34.23949670791626\n",
      "Total Time: 4806.901815891266\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 150 / 200\n",
      "Score: -495.55000967811793\n",
      "Score: -349.89001227077097\n",
      "Training Team 1:\n",
      "Loss: 0.28 0.28 0.27 0.26 0.48 0.52 0.56 0.56 0.37 0.19 0.31 0.18 0.45 0.73 0.65 0.28 0.27 0.58 0.58 0.38\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.11 0.16 0.14 0.13 0.11 0.11 0.09 0.1 0.07 0.09 0.1 0.1 0.12 0.17 0.11 0.13 0.22 0.15 0.13\n",
      "Time: 33.146355390548706\n",
      "Total Time: 4840.048220396042\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 151 / 200\n",
      "Score: -1191.9550374839455\n",
      "Score: -15.220012518577278\n",
      "Training Team 1:\n",
      "Loss: 0.22 0.26 0.34 0.37 0.37 0.37 0.48 0.47 0.31 0.19 0.28 0.43 0.44 0.29 0.16 0.29 0.55 0.55 0.5 0.27\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.12 0.23 0.31 0.38 0.51 0.48 0.43 0.48 0.41 0.2 0.15 0.13 0.16 0.21 0.15 0.2 0.14 0.14 0.1\n",
      "Time: 31.247923374176025\n",
      "Total Time: 4871.296182870865\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 152 / 200\n",
      "Score: 139.7699933089316\n",
      "Score: -672.0000023860484\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.18 0.16 0.18 0.42 0.43 0.43 0.5 0.35 0.2 0.34 0.53 0.44 0.16 0.28 0.53 0.49 0.27 0.19 0.34\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.13 0.22 0.25 0.25 0.27 0.26 0.26 0.3 0.18 0.18 0.13 0.17 0.17 0.19 0.29 0.22 0.19 0.11 0.14\n",
      "Time: 30.674282789230347\n",
      "Total Time: 4901.970510005951\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 153 / 200\n",
      "Score: -117.09000577963889\n",
      "Score: -86.22501369472593\n",
      "Training Team 1:\n",
      "Loss: 0.33 0.33 0.33 0.4 0.38 0.24 0.18 0.17 0.23 0.28 0.26 0.16 0.12 0.17 0.22 0.18 0.16 0.18 0.17 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.13 0.15 0.17 0.17 0.25 0.26 0.27 0.18 0.18 0.18 0.13 0.13 0.14 0.12 0.11 0.12 0.14 0.1 0.13 0.13\n",
      "Time: 30.14807629585266\n",
      "Total Time: 4932.118625879288\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 154 / 200\n",
      "Score: 28.354997776448727\n",
      "Score: -346.9750092122704\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.15 0.13 0.14 0.19 0.19 0.25 0.16 0.16 0.2 0.24 0.3 0.2 0.16 0.2 0.23 0.18 0.15 0.13 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.13 0.14 0.13 0.14 0.12 0.12 0.13 0.09 0.09 0.13 0.09 0.1 0.11 0.09 0.09 0.09 0.1 0.12 0.14 0.16\n",
      "Time: 32.649436712265015\n",
      "Total Time: 4964.768107891083\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 155 / 200\n",
      "Score: -72.51500684395432\n",
      "Score: 107.09499295428395\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.22 0.22 0.25 0.27 0.23 0.16 0.13 0.14 0.16 0.13 0.16 0.15 0.11 0.12 0.14 0.1 0.17 0.16 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.1 0.12 0.11 0.12 0.12 0.11 0.12 0.08 0.08 0.13 0.1 0.12 0.11 0.1 0.11 0.12 0.08 0.11 0.12\n",
      "Time: 29.94923210144043\n",
      "Total Time: 4994.717381000519\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 156 / 200\n",
      "Score: 166.53999258577824\n",
      "Score: 84.57999282516539\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.12 0.11 0.15 0.16 0.15 0.12 0.17 0.12 0.13 0.17 0.21 0.18 0.13 0.16 0.24 0.26 0.31 0.16 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.09 0.08 0.12 0.13 0.1 0.11 0.11 0.13 0.1 0.09 0.11 0.11 0.09 0.06 0.08 0.09 0.08 0.06 0.09\n",
      "Time: 29.709283113479614\n",
      "Total Time: 5024.426708698273\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 157 / 200\n",
      "Score: 4.319998097606003\n",
      "Score: -519.5150117930025\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.17 0.18 0.18 0.18 0.31 0.4 0.41 0.4 0.31 0.32 0.26 0.25 0.12 0.21 0.19 0.19 0.17 0.19 0.17\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.09 0.08 0.12 0.13 0.09 0.1 0.1 0.1 0.1 0.09 0.13 0.09 0.09 0.08 0.13 0.05 0.1 0.1 0.08\n",
      "Time: 32.92078518867493\n",
      "Total Time: 5057.347551107407\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 158 / 200\n",
      "Score: -389.67000565025955\n",
      "Score: 123.85999082680792\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.22 0.2 0.21 0.21 0.19 0.2 0.17 0.21 0.16 0.21 0.18 0.14 0.16 0.15 0.2 0.14 0.18 0.13 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.14 0.09 0.1 0.12 0.14 0.15 0.17 0.11 0.11 0.11 0.11 0.08 0.11 0.1 0.09 0.1 0.08 0.07 0.11\n",
      "Time: 27.70604133605957\n",
      "Total Time: 5085.053628921509\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 159 / 200\n",
      "Score: 102.16499572340399\n",
      "Score: 115.72999356687069\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.19 0.21 0.22 0.18 0.23 0.27 0.28 0.26 0.2 0.16 0.15 0.17 0.13 0.14 0.12 0.11 0.14 0.12 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.1 0.12 0.14 0.16 0.15 0.12 0.12 0.11 0.1 0.12 0.16 0.1 0.09 0.12 0.13 0.14 0.14 0.16 0.12\n",
      "Time: 28.097900867462158\n",
      "Total Time: 5113.151566743851\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 160 / 200\n",
      "Score: 104.60999427735806\n",
      "Score: 121.00998854078352\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.17 0.16 0.14 0.15 0.19 0.14 0.17 0.15 0.19 0.14 0.19 0.12 0.15 0.12 0.15 0.14 0.11 0.11 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.12 0.12 0.13 0.15 0.1 0.13 0.1 0.11 0.13 0.12 0.12 0.11 0.14 0.16 0.12 0.11 0.11 0.12 0.12\n",
      "Time: 27.073038578033447\n",
      "Total Time: 5140.224651098251\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 161 / 200\n",
      "Score: -385.0000024260953\n",
      "Score: -711.2000040821731\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.1 0.1 0.09 0.14 0.1 0.1 0.11 0.05 0.1 0.1 0.08 0.1 0.1 0.15 0.07 0.11 0.1 0.12 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.1 0.12 0.12 0.11 0.15 0.1 0.1 0.1 0.09 0.11 0.11 0.14 0.11 0.13 0.18 0.22 0.21 0.13 0.19\n",
      "Time: 31.569222927093506\n",
      "Total Time: 5171.793953180313\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 162 / 200\n",
      "Score: -280.6550037013367\n",
      "Score: -127.09500903263688\n",
      "Training Team 1:\n",
      "Loss: 0.25 0.25 0.29 0.24 0.26 0.23 0.26 0.16 0.16 0.14 0.15 0.16 0.19 0.14 0.13 0.14 0.1 0.15 0.12 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.37 0.51 0.33 0.25 0.23 0.25 0.27 0.34 0.31 0.3 0.26 0.32 0.29 0.25 0.26 0.35 0.32 0.31 0.2 0.3\n",
      "Time: 31.774637937545776\n",
      "Total Time: 5203.568653345108\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 163 / 200\n",
      "Score: -117.1650078324601\n",
      "Score: -555.8150071771815\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.12 0.15 0.13 0.17 0.14 0.15 0.1 0.13 0.12 0.13 0.13 0.13 0.12 0.1 0.15 0.11 0.16 0.15 0.09\n",
      "Training Team 2:\n",
      "Loss: 0.27 0.25 0.29 0.22 0.25 0.23 0.22 0.25 0.16 0.18 0.22 0.21 0.24 0.16 0.2 0.23 0.19 0.15 0.18 0.18\n",
      "Time: 33.09801650047302\n",
      "Total Time: 5236.666707277298\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 164 / 200\n",
      "Score: 119.26999658811837\n",
      "Score: 57.819991670548916\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.14 0.18 0.33 0.35 0.36 0.49 0.35 0.41 0.32 0.18 0.14 0.18 0.19 0.2 0.17 0.2 0.14 0.16 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.23 0.24 0.2 0.24 0.21 0.2 0.17 0.21 0.16 0.19 0.22 0.22 0.17 0.16 0.24 0.19 0.19 0.2 0.23\n",
      "Time: 30.522286415100098\n",
      "Total Time: 5267.189036846161\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 165 / 200\n",
      "Score: 104.47499340586364\n",
      "Score: 105.55999344401062\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.25 0.16 0.14 0.19 0.25 0.25 0.12 0.14 0.13 0.17 0.24 0.15 0.22 0.14 0.1 0.07 0.12 0.13 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.18 0.27 0.21 0.22 0.23 0.18 0.22 0.19 0.16 0.2 0.16 0.18 0.24 0.23 0.19 0.19 0.18 0.15 0.18 0.18\n",
      "Time: 30.08994698524475\n",
      "Total Time: 5297.279021263123\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 166 / 200\n",
      "Score: -181.55000843759626\n",
      "Score: -715.2100048819557\n",
      "Training Team 1:\n",
      "Loss: 0.1 0.13 0.13 0.09 0.14 0.16 0.14 0.14 0.09 0.13 0.12 0.11 0.13 0.14 0.1 0.18 0.12 0.15 0.12 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.19 0.22 0.18 0.19 0.16 0.21 0.16 0.16 0.2 0.14 0.18 0.13 0.18 0.17 0.18 0.23 0.17 0.18 0.26\n",
      "Time: 33.01754546165466\n",
      "Total Time: 5330.296603679657\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 167 / 200\n",
      "Score: 158.18499424215406\n",
      "Score: 97.56499520689249\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.12 0.18 0.13 0.14 0.15 0.15 0.11 0.11 0.17 0.19 0.12 0.16 0.15 0.14 0.14 0.19 0.11 0.11 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.28 0.28 0.24 0.22 0.21 0.19 0.19 0.18 0.18 0.15 0.17 0.19 0.2 0.2 0.21 0.21 0.18 0.23 0.22 0.15\n",
      "Time: 28.763137817382812\n",
      "Total Time: 5359.059784650803\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 168 / 200\n",
      "Score: -91.22500966209918\n",
      "Score: -157.45500721782446\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.2 0.19 0.3 0.29 0.23 0.3 0.23 0.22 0.21 0.13 0.13 0.15 0.17 0.19 0.18 0.16 0.16 0.14 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.22 0.2 0.22 0.29 0.3 0.18 0.3 0.19 0.23 0.17 0.23 0.23 0.26 0.22 0.17 0.28 0.29 0.25 0.3 0.25\n",
      "Time: 31.958245992660522\n",
      "Total Time: 5391.018068313599\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 169 / 200\n",
      "Score: -111.67500450182706\n",
      "Score: 131.57999452669173\n",
      "Training Team 1:\n",
      "Loss: 0.21 0.21 0.24 0.15 0.17 0.13 0.17 0.15 0.12 0.2 0.16 0.16 0.14 0.12 0.19 0.14 0.15 0.15 0.17 0.1\n",
      "Training Team 2:\n",
      "Loss: 0.25 0.23 0.23 0.15 0.23 0.16 0.18 0.22 0.21 0.17 0.15 0.25 0.21 0.19 0.14 0.27 0.28 0.19 0.14 0.2\n",
      "Time: 32.75071358680725\n",
      "Total Time: 5423.768827676773\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 170 / 200\n",
      "Score: -214.06001050490886\n",
      "Score: -57.470005630515516\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.21 0.2 0.19 0.23 0.16 0.14 0.18 0.14 0.19 0.13 0.18 0.14 0.14 0.18 0.12 0.12 0.14 0.14 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.22 0.21 0.17 0.26 0.23 0.23 0.24 0.22 0.15 0.19 0.18 0.17 0.18 0.2 0.23 0.18 0.17 0.21 0.23 0.2\n",
      "Time: 32.76467323303223\n",
      "Total Time: 5456.533545732498\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 171 / 200\n",
      "Score: 1.3699933076277375\n",
      "Score: -712.3000041125342\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.13 0.23 0.2 0.2 0.15 0.23 0.18 0.21 0.12 0.15 0.28 0.2 0.22 0.2 0.13 0.18 0.18 0.16 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.15 0.17 0.18 0.18 0.13 0.18 0.14 0.12 0.14 0.13 0.14 0.14 0.14 0.13 0.12 0.12 0.13 0.11 0.08 0.12\n",
      "Time: 32.661661863327026\n",
      "Total Time: 5489.195246219635\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 172 / 200\n",
      "Score: 14.089992985129356\n",
      "Score: -53.960005942732096\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.15 0.16 0.18 0.22 0.17 0.24 0.17 0.15 0.25 0.16 0.18 0.22 0.22 0.15 0.14 0.16 0.18 0.18 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.15 0.1 0.25 0.2 0.15 0.16 0.11 0.13 0.15 0.14 0.11 0.12 0.13 0.16 0.12 0.21 0.19 0.14 0.14\n",
      "Time: 32.54705262184143\n",
      "Total Time: 5521.742343902588\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 173 / 200\n",
      "Score: 70.06999482121319\n",
      "Score: 288.31999511644244\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.19 0.17 0.22 0.11 0.2 0.16 0.19 0.14 0.19 0.15 0.16 0.18 0.16 0.14 0.23 0.16 0.16 0.13 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.24 0.21 0.12 0.18 0.19 0.13 0.16 0.15 0.12 0.17 0.14 0.14 0.14 0.16 0.15 0.13 0.13 0.16 0.15 0.17\n",
      "Time: 28.193897485733032\n",
      "Total Time: 5549.936285972595\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 174 / 200\n",
      "Score: 209.35999559238553\n",
      "Score: 32.46499661542475\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.21 0.22 0.18 0.24 0.19 0.18 0.18 0.18 0.19 0.15 0.22 0.18 0.15 0.19 0.2 0.22 0.2 0.14 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.23 0.21 0.16 0.19 0.12 0.17 0.21 0.2 0.18 0.16 0.18 0.16 0.18 0.2 0.15 0.16 0.12 0.14 0.23\n",
      "Time: 30.84350347518921\n",
      "Total Time: 5580.779834985733\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 175 / 200\n",
      "Score: -46.445004927925766\n",
      "Score: 245.73499695770442\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.2 0.13 0.14 0.19 0.13 0.14 0.19 0.16 0.15 0.21 0.13 0.14 0.16 0.14 0.12 0.19 0.19 0.14 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.21 0.2 0.22 0.17 0.15 0.18 0.19 0.21 0.13 0.16 0.14 0.18 0.22 0.23 0.23 0.17 0.16 0.16 0.21\n",
      "Time: 32.385136127471924\n",
      "Total Time: 5613.165026426315\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 176 / 200\n",
      "Score: 45.064996315166354\n",
      "Score: -408.2299985792488\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.16 0.13 0.22 0.12 0.16 0.17 0.22 0.14 0.13 0.14 0.2 0.17 0.15 0.16 0.19 0.15 0.18 0.13 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.12 0.19 0.19 0.22 0.18 0.15 0.21 0.11 0.16 0.19 0.28 0.2 0.13 0.19 0.15 0.15 0.17 0.18 0.18 0.15\n",
      "Time: 33.412967920303345\n",
      "Total Time: 5646.57803606987\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 177 / 200\n",
      "Score: -270.68499854858965\n",
      "Score: 204.53499427251518\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.16 0.21 0.13 0.16 0.16 0.2 0.11 0.15 0.15 0.2 0.11 0.13 0.15 0.12 0.18 0.17 0.13 0.14 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.2 0.21 0.2 0.21 0.31 0.2 0.23 0.2 0.19 0.17 0.21 0.24 0.2 0.16 0.21 0.18 0.2 0.18 0.18 0.16\n",
      "Time: 34.070029735565186\n",
      "Total Time: 5680.648108959198\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 178 / 200\n",
      "Score: -183.7150017041713\n",
      "Score: 2.544997487217188\n",
      "Training Team 1:\n",
      "Loss: 0.21 0.15 0.12 0.16 0.23 0.14 0.16 0.15 0.16 0.14 0.16 0.18 0.14 0.14 0.15 0.15 0.14 0.14 0.1 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.23 0.21 0.26 0.18 0.26 0.18 0.19 0.22 0.14 0.19 0.2 0.18 0.21 0.16 0.18 0.22 0.18 0.22 0.2 0.15\n",
      "Time: 33.39105534553528\n",
      "Total Time: 5714.039201974869\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 179 / 200\n",
      "Score: 192.37999735493213\n",
      "Score: -255.445002165623\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.2 0.17 0.14 0.15 0.17 0.18 0.17 0.19 0.14 0.17 0.14 0.16 0.13 0.15 0.17 0.14 0.14 0.18 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.21 0.16 0.18 0.17 0.16 0.13 0.24 0.17 0.18 0.17 0.23 0.13 0.13 0.15 0.16 0.16 0.17 0.16 0.13 0.16\n",
      "Time: 30.845044136047363\n",
      "Total Time: 5744.884292125702\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 180 / 200\n",
      "Score: 179.6449963916093\n",
      "Score: 199.31999769248068\n",
      "Training Team 1:\n",
      "Loss: 0.16 0.17 0.17 0.16 0.17 0.14 0.21 0.15 0.21 0.14 0.19 0.21 0.2 0.16 0.19 0.2 0.18 0.21 0.21 0.22\n",
      "Training Team 2:\n",
      "Loss: 0.22 0.24 0.2 0.27 0.15 0.22 0.19 0.22 0.17 0.29 0.24 0.2 0.17 0.21 0.17 0.24 0.2 0.28 0.25 0.21\n",
      "Time: 30.47348642349243\n",
      "Total Time: 5775.357815980911\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 181 / 200\n",
      "Score: 12.104997958056629\n",
      "Score: 290.80499732587487\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.17 0.18 0.16 0.22 0.15 0.14 0.17 0.15 0.13 0.2 0.17 0.14 0.13 0.19 0.14 0.18 0.15 0.13 0.12\n",
      "Training Team 2:\n",
      "Loss: 0.3 0.26 0.26 0.28 0.3 0.31 0.29 0.24 0.18 0.2 0.18 0.16 0.21 0.24 0.18 0.18 0.23 0.17 0.21 0.2\n",
      "Test phase for both teams:\n",
      "Score: -254.56499331630766\n",
      "Score: -261.19999177753925\n",
      "Team 1 Avg Test Score: -254.56\n",
      "Team 2 Avg Test Score: -261.20\n",
      "##########################################################################################\n",
      "Time: 39.776607036590576\n",
      "Total Time: 5815.134449481964\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 182 / 200\n",
      "Score: -326.9899989999831\n",
      "Score: -359.3449983969331\n",
      "Training Team 1:\n",
      "Loss: 0.33 0.24 0.31 0.27 0.32 0.21 0.2 0.19 0.23 0.22 0.22 0.16 0.19 0.25 0.19 0.18 0.23 0.23 0.16 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.42 0.5 0.46 0.57 0.62 0.36 0.29 0.34 0.25 0.38 0.34 0.3 0.4 0.41 0.44 0.35 0.28 0.32 0.39 0.31\n",
      "Test phase for both teams:\n",
      "Score: -403.4900112533942\n",
      "Score: -629.7950065564364\n",
      "Team 1 Avg Test Score: -403.49\n",
      "Team 2 Avg Test Score: -629.80\n",
      "##########################################################################################\n",
      "Time: 44.4320113658905\n",
      "Total Time: 5859.566487073898\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 183 / 200\n",
      "Score: -373.7800096273422\n",
      "Score: -398.1600005766377\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.19 0.21 0.17 0.2 0.2 0.23 0.24 0.22 0.21 0.21 0.11 0.16 0.18 0.17 0.19 0.17 0.13 0.17 0.14\n",
      "Training Team 2:\n",
      "Loss: 0.32 0.3 0.32 0.37 0.34 0.31 0.44 0.35 0.29 0.34 0.25 0.2 0.18 0.26 0.28 0.27 0.23 0.21 0.15 0.2\n",
      "Test phase for both teams:\n",
      "Score: -161.2449950883165\n",
      "Score: -405.49999124836177\n",
      "Team 1 Avg Test Score: -161.24\n",
      "Team 2 Avg Test Score: -405.50\n",
      "##########################################################################################\n",
      "Time: 44.56258821487427\n",
      "Total Time: 5904.1291036605835\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 184 / 200\n",
      "Score: -213.35500138625503\n",
      "Score: -515.7899993658066\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.2 0.24 0.22 0.21 0.28 0.24 0.2 0.17 0.18 0.2 0.14 0.19 0.19 0.18 0.18 0.13 0.19 0.17 0.15\n",
      "Training Team 2:\n",
      "Loss: 0.25 0.26 0.17 0.23 0.23 0.26 0.27 0.17 0.24 0.19 0.16 0.17 0.15 0.19 0.22 0.23 0.16 0.16 0.19 0.14\n",
      "Test phase for both teams:\n",
      "Score: -168.64499567076564\n",
      "Score: -456.50999532360584\n",
      "Team 1 Avg Test Score: -168.64\n",
      "Team 2 Avg Test Score: -456.51\n",
      "##########################################################################################\n",
      "Time: 44.93507790565491\n",
      "Total Time: 5949.064209938049\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 185 / 200\n",
      "Score: -75.67500395979732\n",
      "Score: -348.65499857626855\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.15 0.18 0.19 0.13 0.18 0.18 0.17 0.19 0.18 0.18 0.18 0.11 0.14 0.17 0.12 0.1 0.1 0.17 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.14 0.17 0.12 0.15 0.15 0.21 0.19 0.19 0.16 0.22 0.22 0.18 0.14 0.13 0.17 0.2 0.19 0.16 0.16 0.19\n",
      "Test phase for both teams:\n",
      "Score: -343.8049915675074\n",
      "Score: -200.39999548904598\n",
      "Team 1 Avg Test Score: -343.80\n",
      "Team 2 Avg Test Score: -200.40\n",
      "##########################################################################################\n",
      "Time: 44.87489604949951\n",
      "Total Time: 5993.939132928848\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 186 / 200\n",
      "Score: -545.4999969666824\n",
      "Score: -400.1799993449822\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.15 0.19 0.18 0.16 0.15 0.14 0.16 0.15 0.15 0.14 0.2 0.1 0.15 0.12 0.09 0.13 0.11 0.13 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.16 0.15 0.19 0.19 0.18 0.15 0.16 0.16 0.14 0.15 0.15 0.13 0.12 0.13 0.18 0.15 0.12 0.15 0.17 0.16\n",
      "Test phase for both teams:\n",
      "Score: -7.869997957721353\n",
      "Score: -110.32999556977302\n",
      "Team 1 Avg Test Score: -7.87\n",
      "Team 2 Avg Test Score: -110.33\n",
      "##########################################################################################\n",
      "Time: 44.94597339630127\n",
      "Total Time: 6038.8851318359375\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 187 / 200\n",
      "Score: -35.76500509399921\n",
      "Score: -233.1099998774007\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.16 0.15 0.13 0.15 0.14 0.15 0.16 0.14 0.14 0.17 0.11 0.11 0.12 0.14 0.14 0.16 0.12 0.2 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.11 0.13 0.12 0.12 0.13 0.13 0.17 0.11 0.1 0.13 0.11 0.15 0.09 0.1 0.09 0.11 0.11 0.13 0.1 0.16\n",
      "Test phase for both teams:\n",
      "Score: -193.1749982899055\n",
      "Score: -394.699991020374\n",
      "Team 1 Avg Test Score: -193.17\n",
      "Team 2 Avg Test Score: -394.70\n",
      "##########################################################################################\n",
      "Time: 45.96534776687622\n",
      "Total Time: 6084.850505590439\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 188 / 200\n",
      "Score: -95.49500275216997\n",
      "Score: -435.8849977348\n",
      "Training Team 1:\n",
      "Loss: 0.13 0.21 0.14 0.17 0.21 0.19 0.14 0.15 0.16 0.11 0.17 0.15 0.18 0.21 0.16 0.14 0.2 0.15 0.12 0.11\n",
      "Training Team 2:\n",
      "Loss: 0.09 0.13 0.17 0.09 0.07 0.13 0.11 0.12 0.09 0.12 0.11 0.08 0.11 0.07 0.07 0.07 0.1 0.08 0.09 0.1\n",
      "Test phase for both teams:\n",
      "Score: -52.24499856121838\n",
      "Score: -404.9999909475446\n",
      "Team 1 Avg Test Score: -52.24\n",
      "Team 2 Avg Test Score: -405.00\n",
      "##########################################################################################\n",
      "Time: 44.73717141151428\n",
      "Total Time: 6129.587703704834\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 189 / 200\n",
      "Score: -187.8400047905743\n",
      "Score: -558.3999975193292\n",
      "Training Team 1:\n",
      "Loss: 0.14 0.14 0.14 0.16 0.11 0.16 0.13 0.16 0.13 0.19 0.12 0.14 0.12 0.16 0.12 0.12 0.18 0.13 0.14 0.13\n",
      "Training Team 2:\n",
      "Loss: 0.1 0.09 0.12 0.1 0.09 0.07 0.09 0.08 0.08 0.1 0.11 0.08 0.04 0.08 0.13 0.13 0.08 0.1 0.08 0.12\n",
      "Test phase for both teams:\n",
      "Score: -53.479999392293394\n",
      "Score: -404.9999909475446\n",
      "Team 1 Avg Test Score: -53.48\n",
      "Team 2 Avg Test Score: -405.00\n",
      "##########################################################################################\n",
      "Time: 44.657474517822266\n",
      "Total Time: 6174.245203256607\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 190 / 200\n",
      "Score: -191.16000525001436\n",
      "Score: -586.6999987317249\n",
      "Training Team 1:\n",
      "Loss: 0.19 0.16 0.11 0.21 0.14 0.13 0.2 0.12 0.16 0.19 0.09 0.14 0.15 0.15 0.12 0.16 0.13 0.17 0.12 0.22\n",
      "Training Team 2:\n",
      "Loss: 0.04 0.04 0.05 0.04 0.04 0.05 0.05 0.03 0.06 0.04 0.03 0.04 0.05 0.04 0.04 0.08 0.05 0.03 0.03 0.04\n",
      "Test phase for both teams:\n",
      "Score: -35.904997596517205\n",
      "Score: -390.20999132469296\n",
      "Team 1 Avg Test Score: -35.90\n",
      "Team 2 Avg Test Score: -390.21\n",
      "##########################################################################################\n",
      "Time: 45.03158926963806\n",
      "Total Time: 6219.276820898056\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 191 / 200\n",
      "Score: -10.195002813823521\n",
      "Score: -528.1349981920794\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.12 0.15 0.19 0.18 0.13 0.14 0.18 0.2 0.17 0.17 0.19 0.14 0.11 0.17 0.17 0.21 0.1 0.12 0.19\n",
      "Training Team 2:\n",
      "Loss: 0.06 0.04 0.05 0.04 0.05 0.02 0.05 0.03 0.04 0.04 0.05 0.03 0.06 0.04 0.02 0.04 0.04 0.03 0.05 0.05\n",
      "Test phase for both teams:\n",
      "Score: 18.854999912902713\n",
      "Score: -405.2999909603968\n",
      "Team 1 Avg Test Score: 18.85\n",
      "Team 2 Avg Test Score: -405.30\n",
      "##########################################################################################\n",
      "Time: 44.50822997093201\n",
      "Total Time: 6263.785098314285\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 192 / 200\n",
      "Score: -10.40500466618687\n",
      "Score: -514.6149981962517\n",
      "Training Team 1:\n",
      "Loss: 0.18 0.12 0.19 0.19 0.16 0.15 0.2 0.2 0.17 0.15 0.15 0.13 0.21 0.22 0.2 0.18 0.23 0.18 0.18 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.01 0.03 0.02 0.05 0.03 0.06 0.06 0.02 0.03 0.05 0.03 0.05 0.03 0.04 0.04 0.03 0.05 0.04 0.03 0.03\n",
      "Test phase for both teams:\n",
      "Score: -106.4549962375313\n",
      "Score: -405.499990968965\n",
      "Team 1 Avg Test Score: -106.45\n",
      "Team 2 Avg Test Score: -405.50\n",
      "##########################################################################################\n",
      "Time: 44.551488161087036\n",
      "Total Time: 6308.33661365509\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 193 / 200\n",
      "Score: -95.75000443868339\n",
      "Score: -494.49500054400414\n",
      "Training Team 1:\n",
      "Loss: 0.15 0.25 0.18 0.15 0.19 0.21 0.19 0.21 0.19 0.12 0.16 0.12 0.15 0.13 0.13 0.19 0.19 0.17 0.22 0.16\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.03 0.05 0.04 0.04 0.03 0.03 0.03 0.03 0.05 0.02 0.03 0.04 0.04 0.04 0.04 0.04 0.05 0.04 0.03\n",
      "Test phase for both teams:\n",
      "Score: 324.5449965726584\n",
      "Score: -318.2899933857843\n",
      "Team 1 Avg Test Score: 324.54\n",
      "Team 2 Avg Test Score: -318.29\n",
      "##########################################################################################\n",
      "Time: 40.841583013534546\n",
      "Total Time: 6349.178224563599\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 194 / 200\n",
      "Score: -178.94000408798456\n",
      "Score: -471.05999810248613\n",
      "Training Team 1:\n",
      "Loss: 0.23 0.14 0.13 0.18 0.18 0.2 0.16 0.18 0.1 0.19 0.18 0.2 0.19 0.16 0.22 0.14 0.12 0.21 0.14 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.04 0.02 0.02 0.04 0.03 0.02 0.05 0.03 0.05 0.03 0.02 0.04 0.03 0.03 0.02 0.03 0.02 0.03 0.01\n",
      "Test phase for both teams:\n",
      "Score: -36.899997170083225\n",
      "Score: -404.9999909475446\n",
      "Team 1 Avg Test Score: -36.90\n",
      "Team 2 Avg Test Score: -405.00\n",
      "##########################################################################################\n",
      "Time: 45.160013914108276\n",
      "Total Time: 6394.338265419006\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 195 / 200\n",
      "Score: -179.6800034539774\n",
      "Score: -523.5399971464649\n",
      "Training Team 1:\n",
      "Loss: 0.24 0.21 0.27 0.18 0.17 0.19 0.2 0.22 0.24 0.16 0.14 0.15 0.18 0.2 0.22 0.21 0.13 0.25 0.22 0.2\n",
      "Training Team 2:\n",
      "Loss: 0.02 0.03 0.04 0.01 0.04 0.02 0.03 0.03 0.04 0.02 0.02 0.01 0.01 0.02 0.01 0.02 0.02 0.02 0.02 0.03\n",
      "Test phase for both teams:\n",
      "Score: -62.404996909201145\n",
      "Score: -391.424991235137\n",
      "Team 1 Avg Test Score: -62.40\n",
      "Team 2 Avg Test Score: -391.42\n",
      "##########################################################################################\n",
      "Time: 45.59310579299927\n",
      "Total Time: 6439.9313995838165\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 196 / 200\n",
      "Score: 216.61499607842416\n",
      "Score: -530.1949966615066\n",
      "Training Team 1:\n",
      "Loss: 0.17 0.22 0.25 0.26 0.22 0.17 0.27 0.28 0.26 0.22 0.24 0.23 0.19 0.21 0.21 0.25 0.22 0.34 0.24 0.18\n",
      "Training Team 2:\n",
      "Loss: 0.03 0.03 0.01 0.02 0.03 0.01 0.02 0.02 0.01 0.02 0.01 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.01 0.02\n",
      "Test phase for both teams:\n",
      "Score: -42.92999682109803\n",
      "Score: -404.9999909475446\n",
      "Team 1 Avg Test Score: -42.93\n",
      "Team 2 Avg Test Score: -405.00\n",
      "##########################################################################################\n",
      "Time: 40.88731050491333\n",
      "Total Time: 6480.818738222122\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 197 / 200\n",
      "Score: -129.04000112507492\n",
      "Score: -543.9349971413612\n",
      "Training Team 1:\n",
      "Loss: 0.2 0.18 0.25 0.24 0.22 0.27 0.25 0.18 0.19 0.18 0.19 0.26 0.18 0.25 0.22 0.18 0.21 0.24 0.2 0.22\n",
      "Training Team 2:\n",
      "Loss: 0.01 0.01 0.01 0.01 0.01 0.02 0.02 0.02 0.02 0.01 0.02 0.02 0.01 0.03 0.03 0.03 0.01 0.01 0.01 0.01\n",
      "Test phase for both teams:\n",
      "Score: -58.07499614916742\n",
      "Score: -401.8299911711365\n",
      "Team 1 Avg Test Score: -58.07\n",
      "Team 2 Avg Test Score: -401.83\n",
      "##########################################################################################\n",
      "Time: 45.33131694793701\n",
      "Total Time: 6526.150090456009\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 198 / 200\n",
      "Score: -127.79000446293503\n",
      "Score: -549.9949974985793\n",
      "Training Team 1:\n",
      "Loss: 0.23 0.23 0.24 0.23 0.24 0.15 0.18 0.26 0.21 0.19 0.2 0.23 0.2 0.22 0.18 0.17 0.15 0.25 0.23 0.24\n",
      "Training Team 2:\n",
      "Loss: 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.03 0.0 0.02 0.01 0.01 0.02 0.01 0.03 0.01 0.01 0.02 0.02\n",
      "Test phase for both teams:\n",
      "Score: 297.12999671231955\n",
      "Score: -406.5999910160899\n",
      "Team 1 Avg Test Score: 297.13\n",
      "Team 2 Avg Test Score: -406.60\n",
      "##########################################################################################\n",
      "Time: 40.841350078582764\n",
      "Total Time: 6566.991479635239\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 199 / 200\n",
      "Score: 222.43499656487256\n",
      "Score: -562.5049987537786\n",
      "Training Team 1:\n",
      "Loss: 0.23 0.21 0.29 0.18 0.23 0.21 0.21 0.21 0.32 0.24 0.24 0.21 0.24 0.27 0.22 0.23 0.26 0.25 0.18 0.25\n",
      "Training Team 2:\n",
      "Loss: 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.02 0.01 0.03 0.01 0.02 0.02 0.0 0.01 0.01\n",
      "Test phase for both teams:\n",
      "Score: -43.865001901984215\n",
      "Score: -182.62999544292688\n",
      "Team 1 Avg Test Score: -43.87\n",
      "Team 2 Avg Test Score: -182.63\n",
      "##########################################################################################\n",
      "Time: 41.60140347480774\n",
      "Total Time: 6608.59291601181\n",
      "------------------------------------------------------------------------------------------\n",
      "Episodes 200 / 200\n",
      "Score: 276.14999569021165\n",
      "Score: -415.8649981152266\n",
      "Training Team 1:\n",
      "Loss: 0.3 0.31 0.26 0.26 0.23 0.19 0.25 0.19 0.21 0.26 0.23 0.23 0.3 0.28 0.17 0.21 0.26 0.26 0.23 0.21\n",
      "Training Team 2:\n",
      "Loss: 0.01 0.01 0.03 0.03 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.02 0.01 0.01 0.02 0.01 0.02 0.01\n",
      "Test phase for both teams:\n",
      "Score: 40.39000233169645\n",
      "Score: -405.79999098181725\n",
      "Team 1 Avg Test Score: 40.39\n",
      "Team 2 Avg Test Score: -405.80\n",
      "##########################################################################################\n",
      "Time: 40.56568002700806\n",
      "Total Time: 6649.158649921417\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "save_name_team1 = 'vdn_blue'\n",
    "save_name_team2 = 'vdn_red'\n",
    "\n",
    "# Hyperparameters\n",
    "hp = VdnHyperparameters(\n",
    "    lr=0.002,\n",
    "    gamma=0.99,\n",
    "    batch_size=512,\n",
    "    buffer_limit=9000,\n",
    "    max_episodes=200,\n",
    "    max_epsilon=0.9,\n",
    "    min_epsilon=0.1,\n",
    "    episode_min_epsilon=100,\n",
    "    test_episodes=1,\n",
    "    warm_up_steps=3000,\n",
    "    update_iter=20,\n",
    "    chunk_size=1,\n",
    "    update_target_interval=20,\n",
    "    recurrent=True\n",
    ")\n",
    "print(hp)\n",
    "\n",
    "# Create environment\n",
    "env = battle_v4.parallel_env(map_size=45)\n",
    "test_env = battle_v4.parallel_env(map_size=45)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "test_env.reset(seed=seed)\n",
    "team_manager = TeamManager(env.agents)\n",
    "\n",
    "# Create models for two teams\n",
    "q_team1 = VdnQNet(team_manager.get_my_agents(), env.observation_spaces, env.action_spaces).to(device)\n",
    "q_target_team1 = VdnQNet(team_manager.get_my_agents(), env.observation_spaces, env.action_spaces).to(device)\n",
    "\n",
    "q_team2 = VdnQNet(team_manager.get_other_agents(), env.observation_spaces, env.action_spaces).to(device)\n",
    "q_target_team2 = VdnQNet(team_manager.get_other_agents(), env.observation_spaces, env.action_spaces).to(device)\n",
    "\n",
    "# Run training for both teams\n",
    "train_scores_team1, train_scores_team2, test_scores_team1, test_scores_team2, losses_team1, losses_team2 = run_model_train_test(\n",
    "    env, \n",
    "    test_env, \n",
    "    q_team1, q_team2, \n",
    "    q_target_team1, q_target_team2, \n",
    "    save_name_team1, save_name_team2, \n",
    "    team_manager, \n",
    "    hp, \n",
    "    train, \n",
    "    run_episode\n",
    ")\n",
    "\n",
    "# Save data for Team 1\n",
    "save_data(np.array(train_scores_team1), f'{save_name_team1}-train_scores')\n",
    "save_data(np.array(test_scores_team1), f'{save_name_team1}-test_scores')\n",
    "save_data(np.array(losses_team1), f'{save_name_team1}-losses')\n",
    "\n",
    "# Save data for Team 2\n",
    "save_data(np.array(train_scores_team2), f'{save_name_team2}-train_scores')\n",
    "save_data(np.array(test_scores_team2), f'{save_name_team2}-test_scores')\n",
    "save_data(np.array(losses_team2), f'{save_name_team2}-losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28d00c",
   "metadata": {
    "papermill": {
     "duration": 0.047066,
     "end_time": "2024-12-18T18:46:14.663430",
     "exception": false,
     "start_time": "2024-12-18T18:46:14.616364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6700.906589,
   "end_time": "2024-12-18T18:46:17.342005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-18T16:54:36.435416",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
