{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T09:59:54.508099Z",
     "iopub.status.busy": "2024-12-19T09:59:54.507788Z",
     "iopub.status.idle": "2024-12-19T10:00:23.724152Z",
     "shell.execute_reply": "2024-12-19T10:00:23.722389Z",
     "shell.execute_reply.started": "2024-12-19T09:59:54.508072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:23.725865Z",
     "iopub.status.busy": "2024-12-19T10:00:23.725549Z",
     "iopub.status.idle": "2024-12-19T10:00:28.128231Z",
     "shell.execute_reply": "2024-12-19T10:00:28.127034Z",
     "shell.execute_reply.started": "2024-12-19T10:00:23.725836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n",
    "\n",
    "def compute_output_dim(input_dim, kernel_size, stride, padding):\n",
    "    return (input_dim - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "red_path = '/kaggle/input/red-model/red.pt' # 'red.pt'\n",
    "red_final_path = '/kaggle/input/red-model/red_final.pt' # 'red_final.pt'\n",
    "vdn_path = '/kaggle/input/vdn-model/vdn.pth' # 'vdn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:28.129724Z",
     "iopub.status.busy": "2024-12-19T10:00:28.129302Z",
     "iopub.status.idle": "2024-12-19T10:00:28.137706Z",
     "shell.execute_reply": "2024-12-19T10:00:28.136717Z",
     "shell.execute_reply.started": "2024-12-19T10:00:28.129696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, action_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:28.140402Z",
     "iopub.status.busy": "2024-12-19T10:00:28.140098Z",
     "iopub.status.idle": "2024-12-19T10:00:28.162038Z",
     "shell.execute_reply": "2024-12-19T10:00:28.160962Z",
     "shell.execute_reply.started": "2024-12-19T10:00:28.140375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FinalQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            # nn.LayerNorm(120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            # nn.LayerNorm(84),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_layer = nn.Linear(84, action_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        x = self.network(x)\n",
    "        self.last_latent = x\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:28.163830Z",
     "iopub.status.busy": "2024-12-19T10:00:28.163506Z",
     "iopub.status.idle": "2024-12-19T10:00:28.190162Z",
     "shell.execute_reply": "2024-12-19T10:00:28.189286Z",
     "shell.execute_reply.started": "2024-12-19T10:00:28.163801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TeamManager:\n",
    "\n",
    "    def __init__(self, agents, my_team = None):\n",
    "        self.agents = agents\n",
    "        self.teams = self.group_agents()\n",
    "        self.terminated_agents = set()\n",
    "        self.my_team = my_team\n",
    "        self.random_agents = None\n",
    "        self.get_random_agents(1)\n",
    "\n",
    "    def get_teams(self):\n",
    "        \"\"\"\n",
    "        Get the team names.\n",
    "        :return: a list of team names\n",
    "        \"\"\"\n",
    "        return list(self.teams.keys())\n",
    "\n",
    "    def get_my_team(self):\n",
    "        if self.my_team is not None:\n",
    "            return self.my_team\n",
    "        else:\n",
    "            my_team = self.get_teams()[1]\n",
    "        self.my_team = my_team\n",
    "        return my_team\n",
    "\n",
    "    def get_team_agents(self, team):\n",
    "        \"\"\"\n",
    "        Get the agents in a team.\n",
    "        :param team: the team name\n",
    "        :return: a list of agent names in the team\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        return self.teams[team]\n",
    "\n",
    "    def get_my_agents(self):\n",
    "        return self.get_team_agents(self.get_my_team())\n",
    "\n",
    "    def group_agents(self):\n",
    "        \"\"\"\n",
    "        Group agents by their team.\n",
    "        :param agents: a list of agent names in the format of teamname_agentid\n",
    "        :return: a dictionary with team names as keys and a list of agent names as values\n",
    "        \"\"\"\n",
    "        teams = collections.defaultdict(list)\n",
    "        for agent in self.agents:\n",
    "            team, _ = agent.split('_')\n",
    "            teams[team].append(agent)\n",
    "        return teams\n",
    "\n",
    "    def get_info_of_team(self, team, data, default=None):\n",
    "        \"\"\"\n",
    "        Get the information of a team.\n",
    "        :param team: the team name\n",
    "        :param data: the data to get information from\n",
    "        :return: a dictionary with the team name as key and the information as value\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        result = {}\n",
    "        for agent in self.get_team_agents(team):\n",
    "            if agent not in data:\n",
    "                result[agent] = default\n",
    "            else:\n",
    "                result[agent] = data[agent]\n",
    "        return result\n",
    "    \n",
    "    def reset(self):\n",
    "        self.terminated_agents = set()\n",
    "\n",
    "    def is_team_terminated(self, team):\n",
    "        \"\"\"\n",
    "        Check if all agents in a team are terminated.\n",
    "        :param team: the team name\n",
    "        :return: True if all agents in the team are terminated, False otherwise\n",
    "        \"\"\"\n",
    "        assert team in self.teams, f\"Team [{team}] not found.\"\n",
    "        return all(agent in self.terminated_agents for agent in self.teams[team])\n",
    "\n",
    "    def terminate_agent(self, agent):\n",
    "        \"\"\"\n",
    "        Mark an agent as terminated.\n",
    "        :param agent:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.terminated_agents.add(agent)\n",
    "\n",
    "    def has_terminated_teams(self):\n",
    "        \"\"\"\n",
    "        Check if any team is terminated.\n",
    "        \"\"\"\n",
    "        for team in self.teams:\n",
    "            if self.is_team_terminated(team):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_my_terminated_agents(self):\n",
    "        return list(self.terminated_agents.intersection(self.get_my_agents()))\n",
    "\n",
    "    def get_other_team_remains(self):\n",
    "        \"\"\"\n",
    "        Get the remaining agents in the other team.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        my_team = self.get_my_team()\n",
    "        other_team = [team for team in self.teams if team != my_team][0]\n",
    "        return [agent for agent in self.get_team_agents(other_team) if agent not in self.terminated_agents]\n",
    "\n",
    "\n",
    "    def get_random_agents(self, rate):\n",
    "        \"\"\"\n",
    "        Create a random agent list, and return the first n agents.\n",
    "        :param rate: the rate of random agents to return\n",
    "        :return: a list of random agents with the length of rate * num_agents\n",
    "        \"\"\"\n",
    "        num_agents = len(self.get_my_agents())\n",
    "        if self.random_agents is not None:\n",
    "            num_random_agents = int(num_agents * rate)\n",
    "            return self.random_agents[:num_random_agents]\n",
    "        else:\n",
    "            self.random_agents = random.sample(self.get_my_agents(), num_agents)\n",
    "            return self.get_random_agents(rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_terminates_truncates(terminates, truncates):\n",
    "        \"\"\"\n",
    "        Merge terminates and truncates into one dictionary.\n",
    "        :param terminates: a dictionary with agent names as keys and boolean values as values\n",
    "        :param truncates: a dictionary with agent names as keys and boolean values as values\n",
    "        :return: a dictionary with agent names as keys and boolean values as values\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        for agent in terminates:\n",
    "            result[agent] = terminates[agent] or truncates[agent]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:28.191337Z",
     "iopub.status.busy": "2024-12-19T10:00:28.191028Z",
     "iopub.status.idle": "2024-12-19T10:00:28.209247Z",
     "shell.execute_reply": "2024-12-19T10:00:28.208305Z",
     "shell.execute_reply.started": "2024-12-19T10:00:28.191306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VdnQNet(nn.Module):\n",
    "    def __init__(self, agents, observation_spaces, action_spaces, recurrent=False):\n",
    "        super(VdnQNet, self).__init__()\n",
    "        self.agents = agents\n",
    "        self.num_agents = len(agents)\n",
    "        self.recurrent = recurrent\n",
    "        self.hx_size = 32   # latent repr size\n",
    "        self.n_obs = observation_spaces[agents[0]].shape    # observation space size of agents\n",
    "        self.n_act = action_spaces[agents[0]].n  # action space size of agents\n",
    "\n",
    "        stride1, stride2 = 1, 1\n",
    "        padding1, padding2 = 1, 1\n",
    "        kernel_size1, kernel_size2 = 3, 3\n",
    "        pool_kernel_size, pool_stride = 2, 2\n",
    "\n",
    "        height = self.n_obs[0]  # n_obs is a tuple (height, width, channels)\n",
    "        out_dim1 = compute_output_dim(height, kernel_size1, stride1, padding1) // pool_stride\n",
    "        out_dim2 = compute_output_dim(out_dim1, kernel_size2, stride2, padding2) // pool_stride\n",
    "\n",
    "        # Compute the final flattened size\n",
    "        flattened_size = out_dim2 * out_dim2 * 64\n",
    "        self.feature_cnn = nn.Sequential(\n",
    "            nn.Conv2d(self.n_obs[2], 32, kernel_size=kernel_size1, stride=stride1, padding=padding1),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.Conv2d(32, 64, kernel_size=kernel_size2, stride=stride2, padding=padding2),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, self.hx_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if recurrent:\n",
    "            self.gru =  nn.GRUCell(self.hx_size, self.hx_size)  # shape: hx_size, hx_size\n",
    "        self.q_val = nn.Linear(self.hx_size, self.n_act)    # shape: hx_size, n_actions\n",
    "\n",
    "    def forward(self, obs, hidden):\n",
    "        \"\"\"Predict q values for each agent's actions in the batch\n",
    "        :param obs: [batch_size, num_agents, ...n_obs]\n",
    "        :param hidden: [batch_size, num_agents, hx_size]\n",
    "        :return: q_values: [batch_size, num_agents, n_actions], hidden: [batch_size, num_agents, hx_size]\n",
    "        \"\"\"\n",
    "        # TODO: need to have a done_mask param\n",
    "        obs = obs.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "        \n",
    "        batch_size, num_agents, height, width, channels = obs.shape\n",
    "        obs = obs.permute(0, 1, 4, 2, 3)  # (batch_size, num_agents, channels, height, width)\n",
    "        obs = obs.reshape(batch_size * num_agents, channels, height, width)  # (batch_size * num_agents, channels, height, width)\n",
    "        \n",
    "        # Forward qua feature_cnn cho tất cả agents cùng lúc\n",
    "        x = self.feature_cnn(obs)  # (batch_size * num_agents, hx_size)\n",
    "        \n",
    "        # Nếu recurrent, xử lý qua GRU\n",
    "        if self.recurrent:\n",
    "            hidden = hidden.reshape(batch_size * num_agents, -1)  # (batch_size * num_agents, hx_size)\n",
    "            x = self.gru(x, hidden)  # (batch_size * num_agents, hx_size)\n",
    "        \n",
    "        # Dự đoán Q-values cho tất cả agents\n",
    "        q_values = self.q_val(x)  # (batch_size * num_agents, n_actions)\n",
    "        \n",
    "        # Reshape lại để trả về đúng kích thước\n",
    "        q_values = q_values.view(batch_size, num_agents, -1)  # (batch_size, num_agents, n_actions)\n",
    "        \n",
    "        if self.recurrent:\n",
    "            next_hidden = x.view(batch_size, num_agents, -1)  # (batch_size, num_agents, hx_size)\n",
    "        else:\n",
    "            next_hidden = hidden.view(batch_size, num_agents, -1)\n",
    "        \n",
    "        return q_values, next_hidden\n",
    "\n",
    "    def sample_action(self, obs, hidden, epsilon=1e3):\n",
    "        \"\"\"Choose action with epsilon-greedy policy, for each agent in the batch\n",
    "        :param obs: a batch of observations, [batch_size, num_agents, n_obs]\n",
    "        :param hidden: a batch of hidden states, [batch_size, num_agents, hx_size]\n",
    "        :param epsilon: exploration rate\n",
    "        :return: actions: [batch_size, num_agents], hidden: [batch_size, num_agents, hx_size]\n",
    "        \"\"\"\n",
    "        # TODO: need to have a done_mask param\n",
    "        obs = obs.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "        \n",
    "        q_values, hidden = self.forward(obs, hidden)    # [batch_size, num_agents, n_actions], [batch_size, num_agents, hx_size]\n",
    "        # epsilon-greedy action selection: choose random action with epsilon probability\n",
    "        mask = (torch.rand((q_values.shape[0],), device=device) <= epsilon)  # [batch_size]\n",
    "        actions = torch.empty((q_values.shape[0], q_values.shape[1]), device=device)  # [batch_size, num_agents]\n",
    "        actions[mask] = torch.randint(0, q_values.shape[2], actions[mask].shape, device=device).float()\n",
    "        actions[~mask] = q_values[~mask].argmax(dim=2).float()  # choose action with max q value\n",
    "        return actions, hidden   # [batch_size, num_agents], [batch_size, num_agents, hx_size]\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros((batch_size, self.num_agents, self.hx_size), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:00:28.210658Z",
     "iopub.status.busy": "2024-12-19T10:00:28.210311Z",
     "iopub.status.idle": "2024-12-19T10:20:44.173862Z",
     "shell.execute_reply": "2024-12-19T10:20:44.172757Z",
     "shell.execute_reply.started": "2024-12-19T10:00:28.210623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py:58: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/wrappers/base.py:72: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Eval with random policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [08:09<00:00, 16.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': -3.961059815738994, 'average_rewards_blue': 2.53202465689423}\n",
      "====================\n",
      "Eval with trained policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:52<00:00,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 0.9666666666666667, 'average_rewards_red': 2.5313415457264603, 'average_rewards_blue': 2.469069887490352}\n",
      "====================\n",
      "Eval with final trained policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:13<00:00, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.03333333333333333, 'winrate_blue': 0.7666666666666667, 'average_rewards_red': 0.8238024302942241, 'average_rewards_blue': 1.8392366024504563}\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_cycles = 300\n",
    "env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def random_policy(env, agent, obs):\n",
    "    return env.action_space(agent).sample()\n",
    "\n",
    "q_network = QNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "q_network.load_state_dict(\n",
    "    torch.load(red_path, weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "q_network.to(device)\n",
    "\n",
    "\n",
    "final_q_network = FinalQNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "final_q_network.load_state_dict(\n",
    "    torch.load(red_final_path, weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "final_q_network.to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "team_manager = TeamManager(env.agents)\n",
    "\n",
    "vdn_network = VdnQNet(\n",
    "    team_manager.get_my_agents(), env.observation_spaces, env.action_spaces\n",
    ")\n",
    "vdn_network.load_state_dict(\n",
    "    torch.load(vdn_path, weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "vdn_network.to(device)\n",
    "\n",
    "\n",
    "def pretrain_policy(env, agent, obs):\n",
    "    observation = (\n",
    "        torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(observation)\n",
    "    return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "def final_pretrain_policy(env, agent, obs):\n",
    "    observation = (\n",
    "        torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        q_values = final_q_network(observation)\n",
    "    return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "def blue_policy(env, agent, obs):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    hidden = vdn_network.init_hidden(batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        actions, team_hiddens = vdn_network.sample_action(obs_tensor, hidden, epsilon=0)\n",
    "        selected_action = int(actions.item())\n",
    "    return selected_action\n",
    "\n",
    "def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n",
    "    red_win, blue_win = [], []\n",
    "    red_tot_rw, blue_tot_rw = [], []\n",
    "    n_agent_each_team = len(env.env.action_spaces) // 2\n",
    "\n",
    "    for _ in tqdm(range(n_episode)):\n",
    "        env.reset()\n",
    "        n_kill = {\"red\": 0, \"blue\": 0}\n",
    "        red_reward, blue_reward = 0, 0\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            agent_team = agent.split(\"_\")[0]\n",
    "\n",
    "            n_kill[agent_team] += (\n",
    "                reward > 4.5\n",
    "            )  # This assumes default reward settups\n",
    "            if agent_team == \"red\":\n",
    "                red_reward += reward\n",
    "            else:\n",
    "                blue_reward += reward\n",
    "\n",
    "            if termination or truncation:\n",
    "                action = None  # this agent has died\n",
    "            else:\n",
    "                if agent_team == \"red\":\n",
    "                    action = red_policy(env, agent, observation)\n",
    "                else:\n",
    "                    action = blue_policy(env, agent, observation)\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n",
    "        who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n",
    "        red_win.append(who_wins == \"red\")\n",
    "        blue_win.append(who_wins == \"blue\")\n",
    "\n",
    "        red_tot_rw.append(red_reward / n_agent_each_team)\n",
    "        blue_tot_rw.append(blue_reward / n_agent_each_team)\n",
    "\n",
    "    return {\n",
    "        \"winrate_red\": np.mean(red_win),\n",
    "        \"winrate_blue\": np.mean(blue_win),\n",
    "        \"average_rewards_red\": np.mean(red_tot_rw),\n",
    "        \"average_rewards_blue\": np.mean(blue_tot_rw),\n",
    "    }\n",
    "\n",
    "print(\"=\" * 20)\n",
    "print(\"Eval with random policy\")\n",
    "print(\n",
    "    run_eval(\n",
    "        env=env, red_policy=random_policy, blue_policy=blue_policy, n_episode=30\n",
    "    )\n",
    ")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "print(\"Eval with trained policy\")\n",
    "print(\n",
    "    run_eval(\n",
    "        env=env, red_policy=pretrain_policy, blue_policy=blue_policy, n_episode=30\n",
    "    )\n",
    ")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "print(\"Eval with final trained policy\")\n",
    "print(\n",
    "    run_eval(\n",
    "        env=env,\n",
    "        red_policy=final_pretrain_policy,\n",
    "        blue_policy=blue_policy,\n",
    "        n_episode=30,\n",
    "    )\n",
    ")\n",
    "print(\"=\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6334756,
     "sourceId": 10244257,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6334734,
     "sourceId": 10243284,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
