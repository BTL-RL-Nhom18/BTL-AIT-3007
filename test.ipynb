{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'RL (Python 3.9.20)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n RL ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from magent2.environments import battle_v4\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Thêm đường dẫn chứa file torch_model.py và final_torch_model.py nếu cần\n",
    "sys.path.append('/kaggle/input/model-agent') \n",
    "from torch_model import QNetwork  # Thay bằng file chứa class QNetwork của bạn\n",
    "from final_torch_model import QNetwork as FinalQNetwork # Thay bằng file chứa class FinalQNetwork của bạn\n",
    "\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state, action, reward, next_state, done = self.buffer[idx]\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.LongTensor([action]),\n",
    "            torch.FloatTensor([reward]),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor([done])\n",
    "        )\n",
    "    \n",
    "    def can_sample(self, batch_size):\n",
    "        return len(self.buffer) >= batch_size\n",
    "\n",
    "class OpponentManager:\n",
    "    def __init__(self, env, device):\n",
    "        self.random_policy = lambda: env.action_space(\"red_0\").sample()\n",
    "        self.device = device\n",
    "        self.best_model = None\n",
    "        self.is_selfplay = False\n",
    "\n",
    "    def get_action(self, obs, episode):\n",
    "        if episode < RANDOM_PHASE_EPISODES or not self.is_selfplay:\n",
    "            return self.random_policy()\n",
    "        return self.get_selfplay_action(obs)\n",
    "\n",
    "    def get_selfplay_action(self, obs):\n",
    "        if self.best_model is None:\n",
    "            return self.random_policy()\n",
    "        state_tensor = torch.FloatTensor(obs).to(self.device)\n",
    "        state_tensor = state_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.best_model(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def update_best_model(self, model, reward):\n",
    "        global best_reward\n",
    "        if reward > best_reward:\n",
    "            self.best_model = copy.deepcopy(model)\n",
    "            best_reward = reward\n",
    "            self.is_selfplay = True\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Khởi tạo môi trường\n",
    "env = battle_v4.env(map_size=45, max_cycles=1000, step_reward=-0.005, attack_penalty=-0.1, attack_opponent_reward=0.2, dead_penalty=-0.1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Khởi tạo Q-networks\n",
    "q_network = QNetwork(\n",
    "    env.observation_space(\"blue_0\").shape,\n",
    "    env.action_space(\"blue_0\").n\n",
    ").to(device)\n",
    "\n",
    "target_network = QNetwork(\n",
    "    env.observation_space(\"blue_0\").shape,\n",
    "    env.action_space(\"blue_0\").n\n",
    ").to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "# Khởi tạo Opponent Manager\n",
    "opponent = OpponentManager(env, device)\n",
    "\n",
    "# Khởi tạo Optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=1e-4)\n",
    "\n",
    "# Thiết lập hyperparameters\n",
    "batch_size = 512  # Thử nghiệm với các giá trị khác nhau\n",
    "gamma = 0.99\n",
    "target_update_freq = 1000\n",
    "train_freq = 4\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05 # Điều chỉnh epsilon_end\n",
    "epsilon_decay = 0.997 # Điều chỉnh epsilon_decay\n",
    "num_episodes = 500 # Tăng số episodes\n",
    "replay_buffer_capacity = 20000 # Tăng dung lượng replay buffer\n",
    "\n",
    "# Khởi tạo các biến cần thiết\n",
    "epsilon = epsilon_start\n",
    "step_count = 0\n",
    "best_reward = float('-inf')\n",
    "rewards_history = []\n",
    "\n",
    "# Self-play parameters\n",
    "RANDOM_PHASE_EPISODES = 200  # Episodes to train against random\n",
    "MIN_REWARD_THRESHOLD = 50    # Min reward before self-play\n",
    "\n",
    "# Hàm chọn hành động cho agent xanh\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space(\"blue_0\").sample()\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "# Hàm tối ưu mô hình\n",
    "def optimize_model(replay_buffer):\n",
    "    if not replay_buffer.can_sample(batch_size):\n",
    "        return\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        replay_buffer,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for states, actions, rewards, next_states, dones in train_loader:\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "\n",
    "        # Current Q values\n",
    "        current_q_values = q_network(states).gather(1, actions)\n",
    "\n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and optimize\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def train(env, num_episodes, epsilon_start, epsilon_end, epsilon_decay):\n",
    "    start_time = time.time()\n",
    "    time_limit = 7200  # 2 hours\n",
    "\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
    "    epsilon = epsilon_start\n",
    "    step_count = 0\n",
    "    best_reward = float('-inf')\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if time.time() - start_time > time_limit:\n",
    "            print(f\"Time limit of {time_limit/3600:.1f} hours reached\")\n",
    "            torch.save(q_network.state_dict(), \"blue_final.pt\")\n",
    "            print(\"Training stopped. Model saved as 'blue_final.pt'\")\n",
    "            return\n",
    "\n",
    "        env.reset()\n",
    "        episode_reward = 0\n",
    "        done = {agent: False for agent in env.agents}\n",
    "        observations = {}\n",
    "\n",
    "        while not all(done.values()):\n",
    "            for agent in env.agent_iter():\n",
    "                obs, reward, termination, truncation, _ = env.last()\n",
    "                agent_team = agent.split(\"_\")[0]\n",
    "\n",
    "                if agent not in observations:\n",
    "                    observations[agent] = obs\n",
    "\n",
    "                if termination or truncation:\n",
    "                    done[agent] = True\n",
    "                else:\n",
    "                    if agent_team == \"blue\":\n",
    "                        action = select_action(obs, epsilon)\n",
    "                        next_obs = env.observe(agent)\n",
    "                        replay_buffer.add(obs, action, reward, next_obs, termination or truncation)\n",
    "                        episode_reward += reward\n",
    "                        observations[agent] = next_obs\n",
    "                    else:\n",
    "                        action = opponent.get_action(obs, episode)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "                if agent_team == 'blue':\n",
    "                    if step_count % train_freq == 0:\n",
    "                        optimize_model(replay_buffer)\n",
    "                    step_count += 1\n",
    "\n",
    "        # Transition to self-play after random phase\n",
    "        if episode == RANDOM_PHASE_EPISODES:\n",
    "            opponent.best_model = copy.deepcopy(q_network)\n",
    "            opponent.is_selfplay = True\n",
    "            print(\"Transitioning to self-play training\")\n",
    "\n",
    "        # Update best model if performance improves during self-play\n",
    "        if episode >= RANDOM_PHASE_EPISODES:\n",
    "            if opponent.update_best_model(q_network, episode_reward):\n",
    "                print(f\"New best self-play model at episode {episode} with reward {episode_reward:.2f}\")\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        if step_count % target_update_freq == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_reward = np.mean(rewards_history[-10:]) if rewards_history else 0\n",
    "            print(f\"Episode {episode}/{num_episodes}\")\n",
    "            print(f\"Elapsed time: {elapsed_time/3600:.1f} hours\")\n",
    "            print(f\"Avg Reward: {avg_reward:.2f}\")\n",
    "            print(f\"Epsilon: {epsilon:.4f}\")\n",
    "            print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        rewards_history.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            checkpoint_path = f\"blue_checkpoint_{episode+1}.pt\"\n",
    "            torch.save(q_network.state_dict(), checkpoint_path)\n",
    "            print(f\"Model checkpoint saved at episode {episode + 1}\")\n",
    "\n",
    "    torch.save(q_network.state_dict(), \"blue.pt\")\n",
    "    print(\"Training complete. Model saved as 'blue.pt'\")\n",
    "\n",
    "def main():\n",
    "    train(env, num_episodes, epsilon_start, epsilon_end, epsilon_decay)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
